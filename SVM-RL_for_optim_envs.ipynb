{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8994df9d",
   "metadata": {},
   "source": [
    "# Stochastic Variational Method with RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a46171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8829f8",
   "metadata": {},
   "source": [
    "## Expoloring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3a490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Env Name ###### svmEnv-v2\n",
      "###### Observation space ####### \n",
      " Box(-inf, inf, (1,), float32)\n",
      "###### Size of observation space ####### \n",
      " 1\n",
      "###### Action space ####### \n",
      " (50, 3)\n",
      "###### Number of actions ####### \n",
      " 150\n",
      "*****CALL RESET******\n",
      "Action chosen at reset:  [0.]\n",
      "##### State after reset ###### \n",
      " [0.]\n",
      "##### File where will be stored sigmas \n",
      " ./svmCodeSVD/sigmas.dat\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('svm_env:svmEnv-v2', n_pairs = 3, n_basis = 50, file_sigmas =\"./svmCodeSVD/sigmas.dat\" )\n",
    "\n",
    "print('### Env Name ######', env.unwrapped.spec.id)\n",
    "\n",
    "obs_space = env.observation_space\n",
    "\n",
    "print('###### Observation space ####### \\n', obs_space)\n",
    "\n",
    "state_size = env.observation_space.shape[-1]\n",
    "\n",
    "print('###### Size of observation space ####### \\n', state_size)\n",
    "\n",
    "act_space = env.action_space.shape\n",
    "\n",
    "print('###### Action space ####### \\n', act_space)\n",
    "\n",
    "act_size = env.action_space.shape[0]*env.action_space.shape[-1]\n",
    "\n",
    "print('###### Number of actions ####### \\n', act_size)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "print('##### State after reset ###### \\n', state)\n",
    "\n",
    "print('##### File where will be stored sigmas \\n', env.file_sigmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae415ae",
   "metadata": {},
   "source": [
    "# Your codes `DDPG` and `PPO`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e81b78",
   "metadata": {},
   "source": [
    "## Functions for saving and clean `ddpg` alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7012c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all rewards, energies and princip dims in files during episode training\n",
    "def create_run_fold_and_info_ddpg(agent, env):\n",
    "    \n",
    "    # Check if folder exist and creat it\n",
    "    i = 0\n",
    "    while os.path.exists(f'runs_optim_envs/run_{i}/'):\n",
    "        i += 1\n",
    "    name_dir = f'runs_optim_envs/run_{i}/'\n",
    "    os.makedirs(name_dir)\n",
    "    \n",
    "    # Create info.p to store info in pickle file\n",
    "    info = {'alg':agent.name, 'env':env.unwrapped.spec.id , 'basis_size':env.n_basis \\\n",
    "            , 'batch_size':agent.batch_size, 'bootstrap_size':agent.bootstrap_size \\\n",
    "            , 'gamma':agent.gamma, 'tau':agent.tau,'lr_critic':agent.lr_critic \\\n",
    "            , 'lr_actor':agent.lr_actor, 'update_every':agent.update_every \\\n",
    "            , 'transfer_every':agent.transfer_every, 'num_update':agent.num_update \\\n",
    "            , 'add_noise_every':agent.add_noise_every}\n",
    "    \n",
    "    pickle.dump(info, open(name_dir+'info.p', 'wb'))\n",
    "    return name_dir\n",
    "    \n",
    "def save_all(name_run_dir, i_ep, sigmas_i_ep, rew_i_ep, en_i_ep, pri_dim_i_ep \\\n",
    "             , full_dim_i_ep, act_model_i_ep, cr_model_i_ep):\n",
    "    \n",
    "    pickle.dump(sigmas_i_ep, open(name_run_dir+f'sigmas_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(rew_i_ep, open(name_run_dir+f'rew_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(en_i_ep, open(name_run_dir+f'en_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(pri_dim_i_ep, open(name_run_dir+f'pri_dim_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(full_dim_i_ep, open(name_run_dir+f'full_dim_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(act_model_i_ep, open(name_run_dir+f'act_model_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(cr_model_i_ep, open(name_run_dir+f'cr_model_{i_ep}.p', 'wb'))\n",
    "    \n",
    "def rm_useless_file(actor_model_file, critic_model_file, file_sigmas):\n",
    "    os.remove(actor_model_file)\n",
    "    os.remove(critic_model_file)\n",
    "    os.remove(file_sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c58121",
   "metadata": {},
   "source": [
    "## From my `ddpg_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import DDPG_agent\n",
    "agent = DDPG_agent(state_size, act_size, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc90e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run ddpg algs   \n",
    "def run_ddpg(max_t_step = 10, n_episodes=10):\n",
    "    \n",
    "    # Create h5 file and store info about alg and its hypereparams\n",
    "    name_run_dir = create_run_fold_and_info(agent, env)\n",
    "    \n",
    "    for i_ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        rew_i_ep = []\n",
    "        en_i_ep = []\n",
    "        pri_dim_i_ep = []\n",
    "        full_dim_i_ep = []\n",
    "        action_i_episode = []\n",
    "\n",
    "        ## Training loop of each episode\n",
    "        for t_step in range(max_t_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action.reshape((env.n_basis,env.n_pairs)))\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Save rew, energies, princip dims, act and crit models\n",
    "            action_i_episode.append(action.reshape((env.n_basis,env.n_pairs)))\n",
    "            rew_i_ep.append(reward)\n",
    "            en_i_ep.append(state[0])\n",
    "            pri_dim_i_ep.append(env.princp_dim)\n",
    "            full_dim_i_ep.append(env.full_dim)\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        ## Save data during training (to not lose the work done)\n",
    "        save_all(name_run_dir=name_run_dir, i_ep=int(i_ep), sigmas_i_ep=action_i_episode \\\n",
    "                 , rew_i_ep=rew_i_ep, en_i_ep=en_i_ep, pri_dim_i_ep=pri_dim_i_ep \\\n",
    "                 , full_dim_i_ep=full_dim_i_ep, act_model_i_ep='checkpoint_actor.pth' \\\n",
    "                 , cr_model_i_ep='checkpoint_critic.pth')\n",
    "        \n",
    "        print('Episode {} ... Score: {:.3f}'.format(i_ep, np.sum(rew_i_ep)))\n",
    "\n",
    "    rm_useless_file('checkpoint_actor.pth', 'checkpoint_critic.pth', env.file_sigmas)\n",
    "    return name_run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = run_ddpg(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c20c6",
   "metadata": {},
   "source": [
    "## Functions for saving and clean `ppo` alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013143a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all rewards, energies and princip dims in files during episode training\n",
    "def create_run_fold_and_info_ppo(agent, env):\n",
    "    \n",
    "    # Check if folder exist and creat it\n",
    "    i = 0\n",
    "    while os.path.exists(f'runs_optim_envs/run_{i}/'):\n",
    "        i += 1\n",
    "    name_dir = f'runs_optim_envs/run_{i}/'\n",
    "    os.makedirs(name_dir)\n",
    "    \n",
    "    # Create info.p to store info in pickle file\n",
    "    info = {'alg':agent.name, 'env':env.unwrapped.spec.id , 'basis_size':env.n_basis \\\n",
    "            , 'lambda_gae':agent.lambda_gae ,'gamma':agent.gamma \\\n",
    "            , 'clip':agent.clip,'lr_critic':agent.lr_critic \\\n",
    "            , 'lr_actor':agent.lr_actor, 'num_update':agent.num_update \\\n",
    "            , 'add_noise_every':agent.add_noise_every}\n",
    "    \n",
    "    pickle.dump(info, open(name_dir+'info.p', 'wb'))\n",
    "    return name_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af8c45",
   "metadata": {},
   "source": [
    "## From my `ppo_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46aeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo_agent import PPO_agent\n",
    "agent = PPO_agent(state_size, act_size, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7452a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppo(num_episodes = 1, num_trajs = 2, length_traj = 3):\n",
    "    name_run_dir = create_run_fold_and_info_ppo(agent, env)\n",
    "    for k in range(num_episodes):\n",
    "        ## Data for trajectories\n",
    "        trajs_states = []\n",
    "        trajs_acts = []\n",
    "        all_rews = []\n",
    "        trajs_pri_dim = []\n",
    "        trajs_full_dim = []\n",
    "        trajs_log_pol = []\n",
    "        len_trajs = []\n",
    "        \n",
    "        ## Run to collect trajs for a maximum of length_traj\n",
    "        for i in range(num_trajs):\n",
    "            ## Episodic data. Keeps track of rewards per traj\n",
    "            print(f'##### {i}th Traj #####')\n",
    "            ep_rews = []\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "\n",
    "            for t_traj in range(length_traj):\n",
    "                # Track observations in this batch\n",
    "                trajs_states.append(state)\n",
    "\n",
    "                # Calculate action and log policy and perform a step of the env\n",
    "                action, log_policy = agent.act(state)\n",
    "                state, reward, done, info = env.step(action.reshape((env.n_basis,env.n_pairs)))\n",
    "\n",
    "                # Track recent reward, action, and action log policy, pri dim, full dim\n",
    "                ep_rews.append(reward)\n",
    "                trajs_acts.append(action)\n",
    "                trajs_log_pol.append(log_policy)\n",
    "                trajs_pri_dim.append(env.princp_dim)\n",
    "                trajs_full_dim.append(env.full_dim)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            len_trajs.append(1 + t_traj)\n",
    "            all_rews.append(ep_rews)\n",
    "\n",
    "        # Reshape data as tensors\n",
    "        trajs_states = torch.tensor(trajs_states, dtype=torch.float)\n",
    "        trajs_acts = torch.tensor(trajs_acts, dtype=torch.float)\n",
    "        trajs_log_pol = torch.tensor(trajs_log_pol, dtype=torch.float)\n",
    "\n",
    "        # Run step for learning\n",
    "        agent.step(trajs_states, trajs_acts, trajs_log_pol, all_rews, len_trajs)\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        \n",
    "        # Save energies (states), sigmas (actions), rew, pri dim, full dim\n",
    "        # actor, critic models \n",
    "        save_all(name_run_dir=name_run_dir, i_ep=int(k), \\\n",
    "                sigmas_i_ep=trajs_acts.reshape((num_trajs,length_traj,env.n_basis,env.n_pairs)), \\\n",
    "                rew_i_ep=all_rews, \\\n",
    "                en_i_ep=trajs_states.reshape((num_trajs,length_traj)),\\\n",
    "                pri_dim_i_ep=np.reshape(trajs_pri_dim, (num_trajs,length_traj)), \\\n",
    "                full_dim_i_ep=np.reshape(trajs_full_dim, (num_trajs,length_traj)), \\\n",
    "                act_model_i_ep='checkpoint_actor.pth', \\\n",
    "                cr_model_i_ep='checkpoint_critic.pth')\n",
    "        \n",
    "        rm_useless_file('checkpoint_actor.pth', 'checkpoint_critic.pth', env.file_sigmas)\n",
    "        \n",
    "        # Calculate metrics to print\n",
    "        avg_iter_lens = np.mean(len_trajs)\n",
    "        avg_iter_retur = np.mean([np.sum(ep_rews) for ep_rews in all_rews])\n",
    "        \n",
    "        # Print logging statements\n",
    "        print(flush=True)\n",
    "        print(f\"-------------------- Iteration #{agent.k_step} --------------------\", flush=True)\n",
    "        print(f\"Average Episodic Length: {avg_iter_lens}\", flush=True)\n",
    "        print(f\"Average Episodic Return: {avg_iter_retur}\", flush=True)\n",
    "        print(f\"Timesteps So Far: {agent.t_step}\", flush=True)\n",
    "        print(f\"------------------------------------------------------\", flush=True)\n",
    "        print(flush=True)\n",
    "    return name_run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06e061b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### 0th Traj #####\n",
      "*****CALL RESET******\n",
      "Action chosen at reset:  [0.]\n",
      "****CALL STEP****\n",
      "Action chosen at step:  [[110.         78.13669    37.84396  ]\n",
      " [  0.         77.65255    63.174236 ]\n",
      " [ 55.025597  110.          7.86932  ]\n",
      " [  0.         88.853485   63.706417 ]\n",
      " [  2.183628   56.061157   96.90524  ]\n",
      " [ 52.28917    74.58578    50.459785 ]\n",
      " [ 44.00922     1.8432693  47.56466  ]\n",
      " [  8.763382   32.694794    9.682529 ]\n",
      " [ 19.278801   82.059006  106.47586  ]\n",
      " [  0.         33.766212   65.775894 ]\n",
      " [ 77.65105    64.08187    50.381027 ]\n",
      " [  6.21056    29.013102   60.22737  ]\n",
      " [ 62.6778     59.867085   12.85672  ]\n",
      " [ 67.54057   109.179       0.       ]\n",
      " [  0.308918  110.        110.       ]\n",
      " [  1.4039307  73.65468    28.494665 ]\n",
      " [  5.132839   57.072334   38.386787 ]\n",
      " [ 32.023056   42.32032    58.1405   ]\n",
      " [ 75.91939    39.13339    57.515976 ]\n",
      " [105.44864    96.49936    83.87012  ]\n",
      " [  3.453949   31.464834   27.507957 ]\n",
      " [ 68.860565  110.         60.41141  ]\n",
      " [ 64.84862    91.875786   51.731144 ]\n",
      " [ 12.541004  110.          0.       ]\n",
      " [  0.         27.136768   99.14     ]\n",
      " [  9.262127   68.05309     3.9826012]\n",
      " [ 70.94363    89.068535   71.5824   ]\n",
      " [ 29.857273   62.66263    17.524296 ]\n",
      " [ 68.210434   32.651054   50.78108  ]\n",
      " [  0.         36.81826    72.78788  ]\n",
      " [ 33.42189    28.520294   80.54872  ]\n",
      " [ 60.706875   50.642326   47.90277  ]\n",
      " [ 70.04049    77.99267     0.       ]\n",
      " [ 30.420912   64.92648    39.762753 ]\n",
      " [ 59.14412    94.77054    94.888916 ]\n",
      " [ 91.64525    81.8929     44.0098   ]\n",
      " [  9.955189   63.656513  110.       ]\n",
      " [ 55.177666  104.35488    48.217842 ]\n",
      " [ 49.220737   96.97694    45.352325 ]\n",
      " [  0.         31.787022   56.67733  ]\n",
      " [ 59.257965  110.         50.518135 ]\n",
      " [ 54.579758    0.         55.573975 ]\n",
      " [ 70.26236    63.93993    85.65738  ]\n",
      " [ 80.61991    64.46855    53.3789   ]\n",
      " [ 71.76515     7.0026703  27.69197  ]\n",
      " [  7.2345695  68.27594    54.05724  ]\n",
      " [ 66.18649    12.892426   21.105259 ]\n",
      " [ 63.164116   26.153141   78.42905  ]\n",
      " [ 22.11304    70.041504   17.678337 ]\n",
      " [ 62.839813    0.         50.693638 ]]\n",
      "With this action the energy is:  -0.147833\n",
      "With this action the full dim is:  39  and princip dim is:  39\n",
      "#### THE ACTION IS A GOOD ONE ####\n",
      "**** THE AGENT STATE IS THE ENERGY **** -0.147833\n",
      "Set reward :  9.819691124826031\n",
      "****CALL STEP****\n",
      "Action chosen at step:  [[ 85.93093    35.248146  110.       ]\n",
      " [ 41.660557  110.         45.314014 ]\n",
      " [ 71.70592    82.34103    72.9498   ]\n",
      " [ 46.95117    90.13045    45.006687 ]\n",
      " [ 38.34942    61.53682   110.       ]\n",
      " [ 29.510075   83.90959    81.11235  ]\n",
      " [ 12.482674   46.45048    56.17137  ]\n",
      " [  0.         91.09448    85.61223  ]\n",
      " [ 88.81218    13.6492    110.       ]\n",
      " [ 59.698307   31.13701    57.055447 ]\n",
      " [ 88.073975   64.24629    41.96642  ]\n",
      " [ 48.900497   10.2183075   0.       ]\n",
      " [ 67.73959   109.845276   69.01479  ]\n",
      " [  0.         99.84584     5.535076 ]\n",
      " [110.         33.6333      0.       ]\n",
      " [ 70.57209     0.          1.8571854]\n",
      " [ 89.27626    83.62452    23.283102 ]\n",
      " [ 57.096443   43.57773    22.7486   ]\n",
      " [ 78.395485  105.03313    35.22233  ]\n",
      " [ 53.61051    69.74877     4.0054054]\n",
      " [ 16.065735   11.814926   24.588188 ]\n",
      " [ 67.236244   41.463676   47.780613 ]\n",
      " [  0.         21.917248   72.61644  ]\n",
      " [  0.         58.832672   88.74045  ]\n",
      " [ 55.98824    50.24996   110.       ]\n",
      " [110.        110.          0.       ]\n",
      " [ 14.881218  110.         68.63722  ]\n",
      " [ 35.628254   51.219086   40.620117 ]\n",
      " [ 27.405802   94.42226     6.479698 ]\n",
      " [103.96889    85.29163    93.37656  ]\n",
      " [ 59.498077   17.072243   13.871349 ]\n",
      " [ 67.227356   73.66136    94.546646 ]\n",
      " [ 41.54444     8.546707   33.62226  ]\n",
      " [ 41.957935   77.6874     88.10518  ]\n",
      " [ 52.846104   69.23563    43.300465 ]\n",
      " [ 60.668777   75.802635  110.       ]\n",
      " [ 53.34712    22.12901   110.       ]\n",
      " [  0.         87.041756   33.38323  ]\n",
      " [ 27.244398   54.74008    72.63498  ]\n",
      " [ 68.00635    44.06047    43.478374 ]\n",
      " [110.         37.284683   68.19078  ]\n",
      " [  8.066944   38.08117    30.843288 ]\n",
      " [101.43016    44.652122   58.65057  ]\n",
      " [105.863205   53.00567   106.94847  ]\n",
      " [110.         15.378952   80.121826 ]\n",
      " [ 45.790264    0.         49.71208  ]\n",
      " [ 53.18269     0.         25.833286 ]\n",
      " [ 65.249916   54.22012    60.728745 ]\n",
      " [  0.         17.076221   90.43816  ]\n",
      " [ 45.081593   45.002655   39.658623 ]]\n",
      "With this action the energy is:  -0.134275\n",
      "With this action the full dim is:  38  and princip dim is:  38\n",
      "#### THE ACTION IS A GOOD ONE ####\n",
      "**** THE AGENT STATE IS THE ENERGY **** -0.134275\n",
      "Set reward :  8.876875077308325\n",
      "****CALL STEP****\n",
      "Action chosen at step:  [[ 68.25535      0.9250221   99.684555  ]\n",
      " [ 92.74676     60.32109     27.097185  ]\n",
      " [ 34.763283    52.131145    71.985756  ]\n",
      " [  0.2688446   57.29332     13.016499  ]\n",
      " [ 69.057465    67.666534    71.912796  ]\n",
      " [ 83.23039     64.478134    99.20601   ]\n",
      " [ 19.54079     68.42554      4.193802  ]\n",
      " [  0.          92.00926      0.        ]\n",
      " [ 80.56101     24.220667    63.78059   ]\n",
      " [ 40.47599     39.9628       0.        ]\n",
      " [ 81.36264    110.          16.649982  ]\n",
      " [ 62.84633     30.335754    28.050707  ]\n",
      " [ 66.78467     27.554482   106.40591   ]\n",
      " [110.          28.90011     96.77864   ]\n",
      " [  0.          47.190887    24.811073  ]\n",
      " [ 56.68415     37.82414     15.156075  ]\n",
      " [ 58.70227    101.21018     29.682507  ]\n",
      " [ 42.77618     16.806091    50.14603   ]\n",
      " [ 47.71703    110.          82.03596   ]\n",
      " [  0.          34.196598    56.80277   ]\n",
      " [  0.          84.026474   104.145096  ]\n",
      " [107.34572      0.          46.254013  ]\n",
      " [110.           6.7182503   12.415993  ]\n",
      " [110.           0.          22.707207  ]\n",
      " [ 97.483       24.862925    61.834255  ]\n",
      " [ 55.435265    57.866436   104.541084  ]\n",
      " [ 84.935905     3.0162659    0.        ]\n",
      " [ 77.769424    25.681263    69.31015   ]\n",
      " [ 71.69415     68.201164    77.77425   ]\n",
      " [ 67.95871     17.452782   102.79698   ]\n",
      " [ 55.815407    33.123497    77.8588    ]\n",
      " [ 18.452023    77.972595    58.312794  ]\n",
      " [  0.          30.493563   110.        ]\n",
      " [  0.          48.652496    48.035137  ]\n",
      " [ 26.395016    97.46417      0.        ]\n",
      " [ 44.538       55.346977     9.051479  ]\n",
      " [  0.39448547  76.347984    96.132675  ]\n",
      " [ 23.857714    47.06599     60.75261   ]\n",
      " [ 75.434326   108.15563     37.67144   ]\n",
      " [ 69.68033     76.70763     90.25403   ]\n",
      " [ 19.274635     0.          33.613647  ]\n",
      " [ 67.81836     63.369663     4.507366  ]\n",
      " [ 12.563644    76.81365     55.333145  ]\n",
      " [  7.9237976   15.299221     0.8993683 ]\n",
      " [110.          60.199284    61.988453  ]\n",
      " [ 75.044235    67.27419     70.052505  ]\n",
      " [ 29.34698     82.498184    12.663593  ]\n",
      " [ 86.20399      0.          95.80583   ]\n",
      " [ 16.173923    67.06538      0.        ]\n",
      " [110.          33.012363    47.803364  ]]\n",
      "With this action the energy is:  -0.139386\n",
      "With this action the full dim is:  36  and princip dim is:  36\n",
      "#### THE ACTION IS A GOOD ONE ####\n",
      "**** THE AGENT STATE IS THE ENERGY **** -0.139386\n",
      "Set reward :  9.232291275778824\n",
      "##### 1th Traj #####\n",
      "*****CALL RESET******\n",
      "Action chosen at reset:  [0.]\n",
      "****CALL STEP****\n",
      "Action chosen at step:  [[107.7004     76.87915    23.289455 ]\n",
      " [  4.5820427  30.008823   65.62995  ]\n",
      " [ 57.44012    55.203987  102.80352  ]\n",
      " [ 62.75662    62.266483   82.49386  ]\n",
      " [ 28.235168   96.467865  110.       ]\n",
      " [ 73.73873    30.704414   64.05754  ]\n",
      " [ 37.080753  110.         58.929302 ]\n",
      " [ 33.66181    61.725037    0.       ]\n",
      " [108.06504     0.         57.86397  ]\n",
      " [ 80.25954     0.         47.20509  ]\n",
      " [ 84.90811    12.594341   72.86279  ]\n",
      " [ 23.214891   88.82239    75.21674  ]\n",
      " [ 54.20993     4.540718   72.19588  ]\n",
      " [ 48.422573   24.359425   68.36311  ]\n",
      " [ 72.809685   65.43785    44.311123 ]\n",
      " [ 47.397987   59.428577  107.22036  ]\n",
      " [  5.424156   20.50499     0.       ]\n",
      " [ 27.19052    20.076233   78.70939  ]\n",
      " [ 57.62254     0.         72.617645 ]\n",
      " [ 91.72896    75.87444   110.       ]\n",
      " [ 46.13962    30.09231    50.85573  ]\n",
      " [  0.         89.43086     0.       ]\n",
      " [  0.         55.682198   82.71266  ]\n",
      " [110.         35.185833   40.464638 ]\n",
      " [ 52.777813   60.32342     0.       ]\n",
      " [ 36.536674   51.785175  110.       ]\n",
      " [  0.          8.622978   83.17054  ]\n",
      " [ 44.816097  110.         75.970474 ]\n",
      " [ 30.249283   46.225475  110.       ]\n",
      " [ 71.653206   54.323284   29.170877 ]\n",
      " [ 56.907906   59.46551     0.       ]\n",
      " [  0.         35.026203   31.086966 ]\n",
      " [  0.         21.99979    88.86837  ]\n",
      " [ 46.25914    14.312569   24.135468 ]\n",
      " [  0.         62.26313    14.24707  ]\n",
      " [ 55.82269    87.56839    73.32417  ]\n",
      " [ 87.07088     9.642838   41.517086 ]\n",
      " [102.71071    42.85344    85.3457   ]\n",
      " [  0.         23.24994    69.97877  ]\n",
      " [ 38.97111    32.42023    27.809637 ]\n",
      " [ 85.809944   69.219055   86.51391  ]\n",
      " [  0.         53.62221    74.64243  ]\n",
      " [110.         30.7565     28.478481 ]\n",
      " [100.14639    46.92643    62.948486 ]\n",
      " [104.80727    50.881897   44.46588  ]\n",
      " [  3.4749908   0.         65.83887  ]\n",
      " [ 85.5946     47.682083   61.565002 ]\n",
      " [ 68.76042    25.012243   52.220226 ]\n",
      " [  0.         68.53312   110.       ]\n",
      " [ 52.976646   39.44146    54.474426 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  -0.0963499\n",
      "With this action the full dim is:  33  and princip dim is:  33\n",
      "#### THE ACTION IS A GOOD ONE ####\n",
      "**** THE AGENT STATE IS THE ENERGY **** -0.0963499\n",
      "Set reward :  6.239583966251107\n",
      "****CALL STEP****\n",
      "Action chosen at step:  [[ 14.684887   85.512215   80.397736 ]\n",
      " [104.491745   73.52156    28.623278 ]\n",
      " [ 20.096931  107.465805   88.56851  ]\n",
      " [ 16.764183   92.847534   72.75238  ]\n",
      " [ 52.70705   110.         50.467773 ]\n",
      " [  0.        110.          4.530472 ]\n",
      " [ 48.089573   69.38127    85.051636 ]\n",
      " [ 52.680943  110.         82.208824 ]\n",
      " [ 73.14836    76.33143    51.7959   ]\n",
      " [ 37.324566  110.        110.       ]\n",
      " [ 26.625008   37.212273   95.63509  ]\n",
      " [ 36.842407   10.44368    99.31581  ]\n",
      " [ 72.121895   22.835625   45.79837  ]\n",
      " [  9.002365    0.          0.       ]\n",
      " [ 85.323265   82.15439    46.759193 ]\n",
      " [ 34.275597   46.449585  102.62042  ]\n",
      " [ 71.15596     0.8058586  94.61369  ]\n",
      " [ 85.93828    83.107574   74.05152  ]\n",
      " [110.         31.343977   39.233475 ]\n",
      " [ 52.545033   68.15225     0.       ]\n",
      " [107.70152     1.9454002   3.7061615]\n",
      " [ 68.13149    25.420881   16.733028 ]\n",
      " [110.          7.8097153 110.       ]\n",
      " [ 71.07189    57.328205   85.22426  ]\n",
      " [ 52.891743   20.824577   34.67692  ]\n",
      " [ 94.567154   34.830463  101.23326  ]\n",
      " [ 67.446846    6.077156  110.       ]\n",
      " [ 75.268814  100.02214    33.67268  ]\n",
      " [ 48.09203    65.545815   58.89364  ]\n",
      " [ 84.75       67.603615   86.68529  ]\n",
      " [ 85.60001     9.689701    0.8778801]\n",
      " [ 89.03129    59.031868   95.65968  ]\n",
      " [105.05832   108.02455    25.465282 ]\n",
      " [ 52.804863    0.        107.53396  ]\n",
      " [ 71.423744   18.245872  110.       ]\n",
      " [ 19.286701   35.162487  110.       ]\n",
      " [ 87.22567     0.         45.558796 ]\n",
      " [ 49.99276    48.705154   48.303276 ]\n",
      " [ 98.85248    16.054058    0.       ]\n",
      " [ 97.83841    86.14697   110.       ]\n",
      " [ 64.608154   78.8992    110.       ]\n",
      " [ 24.28012    12.133484    0.       ]\n",
      " [ 54.84673    92.8482     72.8522   ]\n",
      " [ 81.13301    48.392807   55.074177 ]\n",
      " [110.         46.515926   61.781006 ]\n",
      " [ 42.145916   30.915873   81.36097  ]\n",
      " [ 79.28274    52.354782   10.886753 ]\n",
      " [ 52.998425    0.          0.       ]\n",
      " [ 67.452446   23.746948  110.       ]\n",
      " [ 15.655819   54.717545   77.43756  ]]\n",
      "With this action the energy is:  -0.0831884\n",
      "With this action the full dim is:  42  and princip dim is:  42\n",
      "#### THE ACTION IS A GOOD ONE ####\n",
      "**** THE AGENT STATE IS THE ENERGY **** -0.0831884\n",
      "Set reward :  5.324340316051646\n",
      "****CALL STEP****\n",
      "Action chosen at step:  [[ 73.3655     47.958015    0.       ]\n",
      " [ 86.421974   57.369682  108.76922  ]\n",
      " [  0.        110.         10.474224 ]\n",
      " [ 59.67973     4.484745   46.231026 ]\n",
      " [ 24.942568   56.535606   15.918716 ]\n",
      " [ 63.489716   11.710739   48.535706 ]\n",
      " [ 17.196331    3.9792137 110.       ]\n",
      " [ 83.17562    72.98155     0.       ]\n",
      " [ 11.81736    13.251278   22.27119  ]\n",
      " [ 89.85701    77.40723    44.960976 ]\n",
      " [ 56.515377   61.586063   19.357697 ]\n",
      " [  0.         51.834816    5.035042 ]\n",
      " [  0.         11.988529   58.643776 ]\n",
      " [110.         63.350986   62.78441  ]\n",
      " [ 47.73468    74.2805     55.984936 ]\n",
      " [ 63.6843      6.70644    35.06948  ]\n",
      " [ 77.27379     6.186043   75.67322  ]\n",
      " [  0.         52.949753   65.737076 ]\n",
      " [  0.        102.59899   103.15956  ]\n",
      " [ 78.57802    65.264114   29.084898 ]\n",
      " [  0.         73.445465   70.439926 ]\n",
      " [ 27.208792    0.          0.       ]\n",
      " [ 34.872192   16.791428   30.162027 ]\n",
      " [110.         21.00261    60.460922 ]\n",
      " [ 94.9559     43.46412    27.77581  ]\n",
      " [ 72.38838    42.122562   75.24816  ]\n",
      " [ 88.13141    36.14171    44.446774 ]\n",
      " [ 55.266792   93.34912    29.852291 ]\n",
      " [ 61.5589    110.          0.       ]\n",
      " [ 16.405174  110.        110.       ]\n",
      " [110.          0.         86.72708  ]\n",
      " [ 15.455578   12.49897    47.40658  ]\n",
      " [110.         36.293457   92.01366  ]\n",
      " [ 34.39206    54.965454   38.799904 ]\n",
      " [  9.970352   49.394825   31.673132 ]\n",
      " [  0.         76.3147     57.692036 ]\n",
      " [ 57.617725    0.          0.       ]\n",
      " [ 17.292297   66.32929    53.664547 ]\n",
      " [ 57.977127   44.585167   89.7083   ]\n",
      " [ 16.55727    15.078339   59.905983 ]\n",
      " [104.34241    99.11265    84.37619  ]\n",
      " [ 63.556923   51.703003   95.93541  ]\n",
      " [ 72.98196    88.75037    72.43322  ]\n",
      " [ 54.04305    68.25724    25.901365 ]\n",
      " [ 23.036264   68.50623    90.07834  ]\n",
      " [ 85.76423    80.52851    86.59816  ]\n",
      " [110.         36.184814    0.       ]\n",
      " [ 87.26537    69.11263     0.       ]\n",
      " [ 83.79026   107.496765  102.26249  ]\n",
      " [ 34.415756   63.18218    38.35652  ]]\n",
      "With this action the energy is:  -0.140149\n",
      "With this action the full dim is:  35  and princip dim is:  35\n",
      "#### THE ACTION IS A GOOD ONE ####\n",
      "**** THE AGENT STATE IS THE ENERGY **** -0.140149\n",
      "Set reward :  9.285349886507248\n",
      "\n",
      "-------------------- Iteration #1 --------------------\n",
      "Average Episodic Length: 3.0\n",
      "Average Episodic Return: 24.38906582336159\n",
      "Timesteps So Far: 6\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-b426c3f413de>:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  trajs_states = torch.tensor(trajs_states, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "name_dir_ppo = run_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15515326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs_optim_envs/run_6/\n"
     ]
    }
   ],
   "source": [
    "print(name_dir_ppo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d72c",
   "metadata": {},
   "source": [
    "## Random search as in original SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "scores = []\n",
    "step = 0\n",
    "score = 0.0\n",
    "\n",
    "while True:\n",
    "    print(\".....STEP.....\", step)\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    step = step + 1\n",
    "    score += reward\n",
    "    scores.append(score)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
