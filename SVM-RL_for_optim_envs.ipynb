{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8994df9d",
   "metadata": {},
   "source": [
    "# Stochastic Variational Method with RL algorithms (ddpg and ppo) for optim envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8829f8",
   "metadata": {},
   "source": [
    "## Expoloring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('svm_env:svmEnv-v2', n_pairs = 3, n_basis = 250, file_sigmas =\"./svmCodeSVD/sigmas.dat\" )\n",
    "\n",
    "print('### Env Name ######', env.unwrapped.spec.id)\n",
    "\n",
    "obs_space = env.observation_space\n",
    "\n",
    "print('###### Observation space ####### \\n', obs_space)\n",
    "\n",
    "state_size = env.observation_space.shape[-1]\n",
    "\n",
    "print('###### Size of observation space ####### \\n', state_size)\n",
    "\n",
    "act_space = env.action_space.shape\n",
    "\n",
    "print('###### Action space ####### \\n', act_space)\n",
    "\n",
    "act_size = env.action_space.shape[0]*env.action_space.shape[-1]\n",
    "\n",
    "print('###### Number of actions ####### \\n', act_size)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "print('##### State after reset ###### \\n', state)\n",
    "\n",
    "print('##### File where will be stored sigmas \\n', env.file_sigmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae415ae",
   "metadata": {},
   "source": [
    "# Your codes `DDPG` and `PPO`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e81b78",
   "metadata": {},
   "source": [
    "## Functions for saving and clean `ddpg` alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7012c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all rewards, energies and princip dims in files during episode training\n",
    "def create_run_fold_and_info_ddpg(agent, env):\n",
    "    \n",
    "    # Check if folder exist and creat it\n",
    "    i = 0\n",
    "    while os.path.exists(f'runs_optim_envs/run_{i}/'):\n",
    "        i += 1\n",
    "    name_dir = f'runs_optim_envs/run_{i}/'\n",
    "    os.makedirs(name_dir)\n",
    "    \n",
    "    # Create info.p to store info in pickle file\n",
    "    info = {'alg':agent.name, 'env':env.unwrapped.spec.id , 'basis_size':env.n_basis \\\n",
    "            , 'batch_size':agent.batch_size, 'bootstrap_size':agent.bootstrap_size \\\n",
    "            , 'gamma':agent.gamma, 'tau':agent.tau,'lr_critic':agent.lr_critic \\\n",
    "            , 'lr_actor':agent.lr_actor, 'update_every':agent.update_every \\\n",
    "            , 'transfer_every':agent.transfer_every, 'num_update':agent.num_update \\\n",
    "            , 'add_noise_every':agent.add_noise_every}\n",
    "    \n",
    "    pickle.dump(info, open(name_dir+'info.p', 'wb'))\n",
    "    return name_dir\n",
    "    \n",
    "def save_all(name_run_dir, i_ep, sigmas_i_ep, rew_i_ep, en_i_ep, pri_dim_i_ep \\\n",
    "             , full_dim_i_ep, act_model_i_ep, cr_model_i_ep):\n",
    "    \n",
    "    pickle.dump(sigmas_i_ep, open(name_run_dir+f'sigmas_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(rew_i_ep, open(name_run_dir+f'rew_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(en_i_ep, open(name_run_dir+f'en_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(pri_dim_i_ep, open(name_run_dir+f'pri_dim_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(full_dim_i_ep, open(name_run_dir+f'full_dim_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(act_model_i_ep, open(name_run_dir+f'act_model_{i_ep}.p', 'wb'))\n",
    "    pickle.dump(cr_model_i_ep, open(name_run_dir+f'cr_model_{i_ep}.p', 'wb'))\n",
    "    \n",
    "def rm_useless_file(actor_model_file, critic_model_file, file_sigmas):\n",
    "    os.remove(actor_model_file)\n",
    "    os.remove(critic_model_file)\n",
    "    os.remove(file_sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c58121",
   "metadata": {},
   "source": [
    "## From my `ddpg_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import DDPG_agent\n",
    "agent = DDPG_agent(state_size, act_size, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc90e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run ddpg algs   \n",
    "def run_ddpg(max_t_step = 10, n_episodes=10):\n",
    "    \n",
    "    # Create h5 file and store info about alg and its hypereparams\n",
    "    name_run_dir = create_run_fold_and_info(agent, env)\n",
    "    \n",
    "    for i_ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        rew_i_ep = []\n",
    "        en_i_ep = []\n",
    "        pri_dim_i_ep = []\n",
    "        full_dim_i_ep = []\n",
    "        action_i_episode = []\n",
    "\n",
    "        ## Training loop of each episode\n",
    "        for t_step in range(max_t_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action.reshape((env.n_basis,env.n_pairs)))\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Save rew, energies, princip dims, act and crit models\n",
    "            action_i_episode.append(action.reshape((env.n_basis,env.n_pairs)))\n",
    "            rew_i_ep.append(reward)\n",
    "            en_i_ep.append(state[0])\n",
    "            pri_dim_i_ep.append(env.princp_dim)\n",
    "            full_dim_i_ep.append(env.full_dim)\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        ## Save data during training (to not lose the work done)\n",
    "        save_all(name_run_dir=name_run_dir, i_ep=int(i_ep), sigmas_i_ep=action_i_episode \\\n",
    "                 , rew_i_ep=rew_i_ep, en_i_ep=en_i_ep, pri_dim_i_ep=pri_dim_i_ep \\\n",
    "                 , full_dim_i_ep=full_dim_i_ep, act_model_i_ep='checkpoint_actor.pth' \\\n",
    "                 , cr_model_i_ep='checkpoint_critic.pth')\n",
    "        \n",
    "        print('Episode {} ... Score: {:.3f}'.format(i_ep, np.sum(rew_i_ep)))\n",
    "\n",
    "    rm_useless_file('checkpoint_actor.pth', 'checkpoint_critic.pth', env.file_sigmas)\n",
    "    return name_run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = run_ddpg(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c20c6",
   "metadata": {},
   "source": [
    "## Functions for saving and clean `ppo` alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013143a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all rewards, energies and princip dims in files during episode training\n",
    "def create_run_fold_and_info_ppo(agent, env):\n",
    "    \n",
    "    # Check if folder exist and creat it\n",
    "    i = 0\n",
    "    while os.path.exists(f'runs_optim_envs/run_{i}/'):\n",
    "        i += 1\n",
    "    name_dir = f'runs_optim_envs/run_{i}/'\n",
    "    os.makedirs(name_dir)\n",
    "    \n",
    "    # Create info.p to store info in pickle file\n",
    "    info = {'alg':agent.name, 'env':env.unwrapped.spec.id , 'basis_size':env.n_basis \\\n",
    "            , 'lambda_gae':agent.lambda_gae ,'gamma':agent.gamma \\\n",
    "            , 'clip':agent.clip,'lr_critic':agent.lr_critic \\\n",
    "            , 'lr_actor':agent.lr_actor, 'num_update':agent.num_update \\\n",
    "            , 'add_noise_every':agent.add_noise_every}\n",
    "    \n",
    "    pickle.dump(info, open(name_dir+'info.p', 'wb'))\n",
    "    return name_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af8c45",
   "metadata": {},
   "source": [
    "## From my `ppo_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46aeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo_agent import PPO_agent\n",
    "agent = PPO_agent(state_size, act_size, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppo(num_episodes = 3, num_trajs = 2, length_traj = 3):\n",
    "    name_run_dir = create_run_fold_and_info_ppo(agent, env)\n",
    "    for k in range(num_episodes):\n",
    "        ## Data for trajectories\n",
    "        trajs_states = []\n",
    "        trajs_acts = []\n",
    "        all_rews = []\n",
    "        trajs_pri_dim = []\n",
    "        trajs_full_dim = []\n",
    "        trajs_log_pol = []\n",
    "        len_trajs = []\n",
    "        \n",
    "        ## Run to collect trajs for a maximum of length_traj\n",
    "        for i in range(num_trajs):\n",
    "            ## Episodic data. Keeps track of rewards per traj\n",
    "            print(f'##### {i}th Traj #####')\n",
    "            ep_rews = []\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "\n",
    "            for t_traj in range(length_traj):\n",
    "                # Track observations in this batch\n",
    "                trajs_states.append(state)\n",
    "\n",
    "                # Calculate action and log policy and perform a step of the env\n",
    "                action, log_policy = agent.act(state)\n",
    "                state, reward, done, info = env.step(action.reshape((env.n_basis,env.n_pairs)))\n",
    "\n",
    "                # Track recent reward, action, and action log policy, pri dim, full dim\n",
    "                ep_rews.append(reward)\n",
    "                trajs_acts.append(action)\n",
    "                trajs_log_pol.append(log_policy)\n",
    "                trajs_pri_dim.append(env.princp_dim)\n",
    "                trajs_full_dim.append(env.full_dim)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            len_trajs.append(1 + t_traj)\n",
    "            all_rews.append(ep_rews)\n",
    "\n",
    "        # Reshape data as tensors\n",
    "        trajs_states = torch.tensor(trajs_states, dtype=torch.float)\n",
    "        trajs_acts = torch.tensor(trajs_acts, dtype=torch.float)\n",
    "        trajs_log_pol = torch.tensor(trajs_log_pol, dtype=torch.float)\n",
    "\n",
    "        # Run step for learning\n",
    "        agent.step(trajs_states, trajs_acts, trajs_log_pol, all_rews, len_trajs)\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        \n",
    "        # Save energies (states), sigmas (actions), rew, pri dim, full dim\n",
    "        # actor, critic models \n",
    "        save_all(name_run_dir=name_run_dir, i_ep=int(k), \\\n",
    "                sigmas_i_ep=trajs_acts.reshape((num_trajs,length_traj,env.n_basis,env.n_pairs)), \\\n",
    "                rew_i_ep=all_rews, \\\n",
    "                en_i_ep=trajs_states.reshape((num_trajs,length_traj)),\\\n",
    "                pri_dim_i_ep=np.reshape(trajs_pri_dim, (num_trajs,length_traj)), \\\n",
    "                full_dim_i_ep=np.reshape(trajs_full_dim, (num_trajs,length_traj)), \\\n",
    "                act_model_i_ep='checkpoint_actor.pth', \\\n",
    "                cr_model_i_ep='checkpoint_critic.pth')\n",
    "        \n",
    "        rm_useless_file('checkpoint_actor.pth', 'checkpoint_critic.pth', env.file_sigmas)\n",
    "        \n",
    "        # Calculate metrics to print\n",
    "        avg_iter_lens = np.mean(len_trajs)\n",
    "        avg_iter_retur = np.mean([np.sum(ep_rews) for ep_rews in all_rews])\n",
    "        \n",
    "        # Print logging statements\n",
    "        print(flush=True)\n",
    "        print(f\"-------------------- Iteration #{agent.k_step} --------------------\", flush=True)\n",
    "        print(f\"Average Episodic Length: {avg_iter_lens}\", flush=True)\n",
    "        print(f\"Average Episodic Return: {avg_iter_retur}\", flush=True)\n",
    "        print(f\"Timesteps So Far: {agent.t_step}\", flush=True)\n",
    "        print(f\"------------------------------------------------------\", flush=True)\n",
    "        print(flush=True)\n",
    "    return name_run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e061b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dir_ppo = run_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15515326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name_dir_ppo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d72c",
   "metadata": {},
   "source": [
    "## Random search as in original SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "scores = []\n",
    "step = 0\n",
    "score = 0.0\n",
    "\n",
    "while True:\n",
    "    print(\".....STEP.....\", step)\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    step = step + 1\n",
    "    score += reward\n",
    "    scores.append(score)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
