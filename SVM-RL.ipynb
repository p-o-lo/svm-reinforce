{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8994df9d",
   "metadata": {},
   "source": [
    "# Stochastic Variational Method with RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a46171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8829f8",
   "metadata": {},
   "source": [
    "## Expoloring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f3a490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Env Name ###### svmEnv-v1\n",
      "###### Observation space ####### \n",
      " Box(-inf, inf, (1,), float32)\n",
      "###### Size of observation space ####### \n",
      " 1\n",
      "###### Action space ####### \n",
      " Box(-1.0, 1.0, (3,), float32)\n",
      "###### Number of actions ####### \n",
      " 3\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "##### State after reset ###### \n",
      " [0.]\n",
      "##### File where will be stored sigmas \n",
      " ./svmCodeSVD/sigmas.dat\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('svm_env:svmEnv-v1', file_sigmas =\"./svmCodeSVD/sigmas.dat\" )\n",
    "\n",
    "print('### Env Name ######', env.unwrapped.spec.id)\n",
    "\n",
    "obs_space = env.observation_space\n",
    "\n",
    "print('###### Observation space ####### \\n', obs_space)\n",
    "\n",
    "state_size = env.observation_space.shape[-1]\n",
    "\n",
    "print('###### Size of observation space ####### \\n', state_size)\n",
    "\n",
    "act_space = env.action_space\n",
    "\n",
    "print('###### Action space ####### \\n', act_space)\n",
    "\n",
    "act_size = env.action_space.shape[-1]\n",
    "\n",
    "print('###### Number of actions ####### \\n', act_size)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "print('##### State after reset ###### \\n', state)\n",
    "\n",
    "print('##### File where will be stored sigmas \\n', env.file_sigmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927566a",
   "metadata": {},
   "source": [
    "## Twin Delayed DDPG (TD3) from `stable_baseline3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for DDPG\n",
    "action_noise = NormalActionNoise(mean=np.zeros(act_size), sigma=0.2 * np.ones(act_size))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, batch_size=64, gamma=1.0, verbose=1, seed=0\n",
    "            , tensorboard_log=logdir )\n",
    "\n",
    "# (policy, env, learning_rate=0.001, buffer_size=1000000,learning_starts=100, batch_size=100, \n",
    "# tau=0.005, gamma=0.99, train_freq=(1, 'episode'),  gradient_steps=- 1, action_noise=None, \n",
    "# replay_buffer_class=None, replay_buffer_kwargs=None,  optimize_memory_usage=False, \n",
    "# tensorboard_log=None, create_eval_env=False, policy_kwargs=None,  verbose=0, seed=None, \n",
    "# device='auto', _init_setup_model=True)\n",
    "\n",
    "model.learn(total_timesteps=300, log_interval = 5, n_eval_episodes = 1)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=4, eval_env=None, eval_freq=- 1,\n",
    "# n_eval_episodes=5, tb_log_name='DDPG', eval_log_path=None, reset_num_timesteps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4411e58",
   "metadata": {},
   "source": [
    "## PPO with GAE from `stable_baseline3` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "total_t_steps = 20\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, gamma = 1.0, tensorboard_log=logdir, batch_size=2, n_steps=2)\n",
    "\n",
    "# classstable_baselines3.ppo.PPO(policy, env, learning_rate=0.0003, n_steps=2048, \n",
    "#         batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2, \n",
    "#         clip_range_vf=None, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, \n",
    "#         use_sde=False, sde_sample_freq=- 1, target_kl=None, tensorboard_log=None, \n",
    "#         create_eval_env=False, policy_kwargs=None, verbose=0, seed=None, device='auto', \n",
    "#         _init_setup_model=True)\n",
    "\n",
    "for i in range(1,10):\n",
    "    model.learn(total_timesteps=total_t_steps, reset_num_timesteps=False)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=1, eval_env=None, eval_freq=- 1, \n",
    "#       n_eval_episodes=5, tb_log_name='PPO', eval_log_path=None, reset_num_timesteps=True)\n",
    "\n",
    "    model.save(f\"{models_dir_ppo}/{total_t_steps*i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c58121",
   "metadata": {},
   "source": [
    "## From my `ddpg_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import DDPG_agent\n",
    "agent = DDPG_agent(state_size, act_size, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a586189",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all rewards, energies and princip dims in files during training\n",
    "def create_info_h5(agent, env):\n",
    "    # Check if file exist and creat it\n",
    "    i = 0\n",
    "    while os.path.exists(f'run_{i}.hdf5'):\n",
    "        i += 1\n",
    "    dataFile = h5py.File(f'run_{i}.hdf5', 'a')\n",
    "    \n",
    "    # Create dataset to store info in hdf5 file\n",
    "    info = {'alg':agent.name, 'env':env.unwrapped.spec.id}\n",
    "    st = h5py.string_dtype(encoding='utf-8')\n",
    "    dataFile.create_dataset('info', dtype=st)\n",
    "    for k in info.keys():\n",
    "        dataFile['info'].attrs[k] = info[k]\n",
    "\n",
    "    # Create dataset to store hyperparams of the model in hdf5 file\n",
    "    hyperparams = {'batch_size':agent.batch_size, 'bootstrap_size':agent.bootstrap_size \\\n",
    "                   , 'gamma':agent.gamma, 'tau':agent.tau,'lr_critic':agent.lr_critic \\\n",
    "                  , 'lr_actor':agent.lr_actor, 'update_every':agent.update_every \\\n",
    "                   , 'transfer_every':agent.transfer_every, 'num_update':agent.num_update \\\n",
    "                  , 'add_noise_every':agent.add_noise_every}\n",
    "    dataFile.create_dataset('hyperparams', dtype='f')\n",
    "    for k in hyperparams.keys():\n",
    "        dataFile['hyperparams'].attrs[k] = hyperparams[k]\n",
    "    \n",
    "    # Create group for rewards, energies, princip dims, actor and critic model\n",
    "    dataFile.create_group('sigmas')\n",
    "    dataFile.create_group('rewards')\n",
    "    dataFile.create_group('energies')\n",
    "    dataFile.create_group('princip_dims')\n",
    "    dataFile.create_group('actor_models')\n",
    "    dataFile.create_group('critic_models')\n",
    "    \n",
    "    # Close and return data file name\n",
    "    dataFile_name = dataFile.filename\n",
    "    dataFile.close()\n",
    "    \n",
    "    return dataFile_name\n",
    "\n",
    "def save_all(dat_file_name, i_ep, sigmas_i_ep, rew_i_ep, en_i_ep, pri_dim_i_ep, act_model_i_ep, cr_model_i_ep):\n",
    "    # Open data file\n",
    "    dat_file = h5py.File(dat_file_name, 'a')\n",
    "    \n",
    "    # Create datasets for rewards, energies, pri dim and store data in it \n",
    "    dat_file['sigmas'].create_dataset(f'sigmas_ep_{i_ep}', dtype='f', data=sigmas_i_ep)\n",
    "    dat_file['rewards'].create_dataset(f'rew_ep_{i_ep}', dtype='f', data=rew_i_ep)\n",
    "    dat_file['energies'].create_dataset(f'en_ep_{i_ep}', dtype='f', data=en_i_ep)\n",
    "    dat_file['princip_dims'].create_dataset(f'pri_dim_ep_{i_ep}', dtype='i', data=pri_dim_i_ep)\n",
    "    \n",
    "    # Store in actor models group the network params at each ep\n",
    "    actor_model = torch.load(act_model_i_ep)\n",
    "    dat_file['actor_models'].create_dataset(f'act_mod_{i_ep}', dtype='f')\n",
    "    for k in actor_model.keys():\n",
    "        dat_file['actor_models'][f'act_mod_{i_ep}'].attrs.create(name=k,data=actor_model[k].cpu().data.numpy())\n",
    "    \n",
    "    # Store in actor models group the network params at each ep\n",
    "    critic_model = torch.load(cr_model_i_ep)\n",
    "    dat_file['critic_models'].create_dataset(f'cri_mod_{i_ep}', dtype='f')\n",
    "    for k in critic_model.keys():\n",
    "        dat_file['critic_models'][f'cri_mod_{i_ep}'].attrs.create(name=k,data=critic_model[k].cpu().data.numpy())\n",
    "    \n",
    "    # Close data file\n",
    "    dat_file.close()\n",
    "    \n",
    "def rm_useless_file(actor_model_file, critic_model_file, file_sigmas):\n",
    "    os.remove(actor_model_file)\n",
    "    os.remove(critic_model_file)\n",
    "    os.remove(file_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc90e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run ddpg algs   \n",
    "def run_ddpg(max_t_step = 10, n_episodes=10):\n",
    "    \n",
    "    # Create h5 file and store info about alg and its hypereparams\n",
    "    dat_file_name = create_info_h5(agent, env)\n",
    "    \n",
    "    for i_ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        rew_i_ep = []\n",
    "        en_i_ep = []\n",
    "        pri_dim_i_ep = []\n",
    "\n",
    "        ## Training loop of each episode\n",
    "        for t_step in range(max_t_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Save rew, energies, princip dims, act and crit models\n",
    "            rew_i_ep.append(reward)\n",
    "            en_i_ep.append(state[0])\n",
    "            pri_dim_i_ep.append(env.princp_dim)\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        ## Save data during training (to not lose the work done)\n",
    "        save_all(dat_file_name=dat_file_name, i_ep=int(i_ep), sigmas_i_ep=env.actions_taken \\\n",
    "                 , rew_i_ep=rew_i_ep, en_i_ep=en_i_ep, pri_dim_i_ep=pri_dim_i_ep \\\n",
    "                 , act_model_i_ep='checkpoint_actor.pth', cr_model_i_ep='checkpoint_critic.pth')\n",
    "        \n",
    "        print('Episode {} ... Score: {:.3f}'.format(i_ep, np.sum(rew_i_ep)))\n",
    "\n",
    "    rm_useless_file('checkpoint_actor.pth', 'checkpoint_critic.pth', env.file_sigmas)\n",
    "    return dat_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051cc27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [41.860283 56.667294 43.198383]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0280577\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.4116538057774957\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [38.077927 54.73852  48.969006]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0211214\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.9293072243940301\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [33.459167 58.16969  63.608486]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0183044\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.7334145554268225\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [19.007923 59.24347  69.03767 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  -0.00264883\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.27633818049472225\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [11.915028 47.740173 71.56185 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.0290432\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.5591121514214858\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [20.947285 48.869137 73.92218 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.0348803\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.9650209544134913\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [15.899086 51.614754 69.148544]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0360202\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.0442889889735607\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [21.336475 36.60012  67.74627 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0410109\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.3913395899727217\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [25.693277 46.49762  64.77911 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.04554\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.706290774573138\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [15.661747 60.33795  74.26865 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0472541\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.8254883691335513\n",
      "Episode 0 ... Score: 7.141\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [62.68191  58.657887 47.92557 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0255368\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.236351771982882\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [62.228233 41.912468 47.94656 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0255339\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2361501075384353\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [62.82571  38.004745 59.98012 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0205484\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8894611117500055\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [69.187645 42.632965 49.907124]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0138875\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.4262656986425633\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [51.052494 51.306683 55.33209 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.011146\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2356232591773182\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [49.471626 37.70897  45.94766 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.010288\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1759583994066887\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [46.62713  29.625044 49.667812]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00382497\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7265227599622044\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [61.5603  51.61205 44.09079]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -8.93639e-05\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.45432208019656173\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [69.631676 51.851337 58.507427]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.00100534\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.39062559353018855\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [58.627544 60.644836 55.309208]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00110329\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.38381420307035974\n",
      "Episode 1 ... Score: -12.155\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [60.04088  57.439426 55.2736  ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0253006\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.219926550680059\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [68.05166  42.113033 82.37902 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.018257\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.73011838485208\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [49.123093 33.403816 82.40752 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0175066\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.6779359713650415\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [58.69238 47.26657 76.00998]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0175004\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.6775048266907078\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [55.027023 56.298412 78.181244]\n",
      "Basis size (it should be the same of full dim) =   5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  0.00952514\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1229095242031661\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [51.409264 26.134151 83.40143 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00481656\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7954773966754409\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [68.41554  33.265205 76.67396 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00086835\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.5209209910868662\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [66.44039  40.85726  74.079796]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.000136535\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.4700309685308728\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [55.79678  33.977745 84.66674 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -2.35766e-05\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.45889689375039566\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [42.315674 34.56806  68.73558 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.000316902\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.43849920277351195\n",
      "Episode 2 ... Score: -11.112\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [57.554035 54.54577  42.4171  ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0268783\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.3296389624051415\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [49.070145 47.19918  70.287636]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.025814\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.255628111293394\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [53.28198  39.513943 62.785187]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0181161\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.7203202744305397\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [60.28878 46.28334 80.54707]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.011861\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2853439756528449\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [48.88475  43.378143 67.4739  ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0116007\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2672428532772084\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [50.100792 66.465645 64.65782 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0109499\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.221986570364944\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [56.1189  62.22692 54.92174]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.0107818\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2102969865334252\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [64.724    60.170155 58.37777 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00967108\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1330581135212636\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [60.68095  43.293274 64.22234 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.00936914\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1120613678812354\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [52.123413 26.923525 67.88875 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.000914382\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.3969507640605112\n",
      "Episode 3 ... Score: -13.933\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [48.402206 58.555634 70.67624 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0245557\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.168126604242831\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [53.10766  63.864815 68.94951 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0233015\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.0809102089930356\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [36.17318  55.466953 67.13164 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0107872\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2106724996368747\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [27.422102 66.046585 61.56483 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.00697121\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.945310601925776\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [33.34698  75.777504 67.79283 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.00564655\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8531944560705753\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [19.787838 93.77282  83.83719 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00184615\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.5889166785970321\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [27.013605 86.12314  86.693794]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.000109775\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.46817009248488084\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [ 8.370487 98.779465 78.9227  ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0168466\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.7109671296517206\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [15.337158 95.24768  84.81055 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0181812\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.8037744977751284\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [ 20.199055  64.94925  108.92732 ]\n",
      "Basis size (it should be the same of full dim) =   10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  -0.0190401\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.8635019430629995\n",
      "Episode 4 ... Score: -5.937\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [55.508633 53.50427  59.20129 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0258545\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2584444595692794\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [51.783684 66.772224 62.753433]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0231507\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.0704236578818342\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [55.50769 50.83954 55.87032]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0135273\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.401217583853077\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [59.348812 50.014668 47.597347]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0135018\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.3994443275312225\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [42.89049  62.129936 53.593697]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0134449\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.395487532052261\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [34.461033 54.17255  62.725254]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00785082\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0064782095048468\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [45.953182 74.21919  77.78125 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00742939\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9771721933589994\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [45.238464 61.90337  63.78032 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00742176\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9766416072517163\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [46.01145  59.458218 54.009315]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.00530456\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8294126549132823\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [58.94865 52.63672 56.616  ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  0.00530404\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8293764943922106\n",
      "Episode 5 ... Score: -13.144\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [56.385674 43.52543  64.97402 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0257219\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2492235266956353\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [54.620605 58.33096  58.264954]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0254486\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.230218391293173\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [41.915146 63.82006  69.77554 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0196456\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.826680884009999\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [23.677784 52.23896  73.46306 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.00272524\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.6500481256550295\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [ 6.9055023 64.30143   89.59926  ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.0188335\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.8491350898827985\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [17.717613 36.648724 81.41019 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.0367053\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.0919304754873824\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [32.915016 40.390488 89.11669 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0372261\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.128146628131372\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [26.881445 47.15426  73.25184 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0376315\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.1563379266756764\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [27.223185 35.574898 72.030556]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0386481\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.227031745373603\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [43.99791  24.614798 60.51675 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0411301\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.3996286940340967\n",
      "Episode 6 ... Score: 4.896\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [63.31885  47.34932  57.275585]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0256812\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2463932705270313\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [43.834244 37.43544  62.005753]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0251481\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2093217824807336\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [51.202515 27.346447 61.8631  ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.00655015\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9160303153814624\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [56.136246 42.94541  55.35661 ]\n",
      "Basis size (it should be the same of full dim) =   4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  0.00595644\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8747440404462399\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [65.6031   22.253376 54.124516]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.00600075\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.04324746165746163\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [68.93989  50.747993 52.00848 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.00685225\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.015965391599753076\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [75.54454  49.316822 75.15082 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.00706679\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.030884388120954398\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [90.93706  36.464302 65.440155]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.00755187\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.0646165911250769\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [76.82293  30.781542 68.04555 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.00788073\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.08748533912527456\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [64.46078  43.581764 60.58945 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00818393\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.10856970448944203\n",
      "Episode 7 ... Score: -5.982\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [59.724472 54.846085 46.039833]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0265269\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.3052027948953526\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [58.405556 70.38603  57.785343]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0235707\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.0996302325947287\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [56.315235 82.75638  39.876892]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0193192\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8039832030902652\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [66.99861 65.5366  44.71692]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0137355\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.415695700175041\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [45.63826 71.32934 38.0363 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0124116\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.3236324043121765\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [65.16565  49.380493 53.551952]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0119575\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2920545338904503\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [64.45428 64.24449 57.59144]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.0117282\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2761091348864806\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [68.04516  61.274384 51.259914]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.0116827\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.272945089292584\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [74.444046 74.272125 39.335995]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.00936089\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1114876673065197\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [80.86971  72.951645 44.64637 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  0.00820022\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0307752980874305\n",
      "Episode 8 ... Score: -14.932\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [55.957203 49.92508  50.250717]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0272358\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.354499320642905\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [55.835945 55.059155 74.588615]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0237176\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.1098455797978826\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [64.01304  46.812416 79.72125 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.01926\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.7998664668450193\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [56.843323 45.21222  83.60827 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0152026\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.5177170472257284\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [27.08994 44.46498 65.86152]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.00592975\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8728880321627006\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [18.081997 29.325264 67.29717 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.0266682\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.3939559253664235\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [26.123812 47.276676 75.94268 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0282957\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.5071314023788922\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [15.332806 50.658882 90.18322 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.030628\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.6693182933381454\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [36.261414 50.90367  73.606964]\n",
      "Basis size (it should be the same of full dim) =   9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  -0.0365573\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.081638634874266\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [36.35948  65.78604  84.630455]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0365812\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.0833006280543573\n",
      "Episode 9 ... Score: 0.081\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dat_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-55d4e08edd5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e42dec2ded87>\u001b[0m in \u001b[0;36mrun_ddpg\u001b[0;34m(max_t_step, n_episodes)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mrm_useless_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint_actor.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint_critic.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_sigmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdat_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dat_file' is not defined"
     ]
    }
   ],
   "source": [
    "all_data = run_ddpg(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d72c",
   "metadata": {},
   "source": [
    "## Random search as in original SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "scores = []\n",
    "step = 0\n",
    "score = 0.0\n",
    "\n",
    "while True:\n",
    "    print(\".....STEP.....\", step)\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    step = step + 1\n",
    "    score += reward\n",
    "    scores.append(score)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
