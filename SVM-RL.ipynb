{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8994df9d",
   "metadata": {},
   "source": [
    "# Stochastic Variational Method with RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a46171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import torch\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8829f8",
   "metadata": {},
   "source": [
    "## Expoloring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f3a490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Env Name ###### svmEnv-v1\n",
      "###### Observation space ####### \n",
      " Box(-inf, inf, (1,), float32)\n",
      "###### Size of observation space ####### \n",
      " 1\n",
      "###### Action space ####### \n",
      " Box(-1.0, 1.0, (3,), float32)\n",
      "###### Number of actions ####### \n",
      " 3\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "##### State after reset ###### \n",
      " [0.]\n",
      "##### File where will be stored sigmas \n",
      " ./svmCodeSVD/sigmas.dat\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('svm_env:svmEnv-v1', file_sigmas =\"./svmCodeSVD/sigmas.dat\" )\n",
    "\n",
    "print('### Env Name ######', env.unwrapped.spec.id)\n",
    "\n",
    "obs_space = env.observation_space\n",
    "\n",
    "print('###### Observation space ####### \\n', obs_space)\n",
    "\n",
    "state_size = env.observation_space.shape[-1]\n",
    "\n",
    "print('###### Size of observation space ####### \\n', state_size)\n",
    "\n",
    "act_space = env.action_space\n",
    "\n",
    "print('###### Action space ####### \\n', act_space)\n",
    "\n",
    "act_size = env.action_space.shape[-1]\n",
    "\n",
    "print('###### Number of actions ####### \\n', act_size)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "print('##### State after reset ###### \\n', state)\n",
    "\n",
    "print('##### File where will be stored sigmas \\n', env.file_sigmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6da707",
   "metadata": {},
   "source": [
    "## Saving folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40786186",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir_ppo = 'models/PPO'\n",
    "models_dir_td3 = 'models/TD3'\n",
    "\n",
    "logdir = 'logs'\n",
    "\n",
    "if not os.path.exists(models_dir_ppo):\n",
    "    os.makedirs(models_dir_ppo)\n",
    "    \n",
    "if not os.path.exists(models_dir_td3):\n",
    "    os.makedirs(models_dir_td3)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927566a",
   "metadata": {},
   "source": [
    "## Twin Delayed DDPG (TD3) from `stable_baseline3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for DDPG\n",
    "action_noise = NormalActionNoise(mean=np.zeros(act_size), sigma=0.2 * np.ones(act_size))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, batch_size=64, gamma=1.0, verbose=1, seed=0\n",
    "            , tensorboard_log=logdir )\n",
    "\n",
    "# (policy, env, learning_rate=0.001, buffer_size=1000000,learning_starts=100, batch_size=100, \n",
    "# tau=0.005, gamma=0.99, train_freq=(1, 'episode'),  gradient_steps=- 1, action_noise=None, \n",
    "# replay_buffer_class=None, replay_buffer_kwargs=None,  optimize_memory_usage=False, \n",
    "# tensorboard_log=None, create_eval_env=False, policy_kwargs=None,  verbose=0, seed=None, \n",
    "# device='auto', _init_setup_model=True)\n",
    "\n",
    "model.learn(total_timesteps=300, log_interval = 5, n_eval_episodes = 1)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=4, eval_env=None, eval_freq=- 1,\n",
    "# n_eval_episodes=5, tb_log_name='DDPG', eval_log_path=None, reset_num_timesteps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4411e58",
   "metadata": {},
   "source": [
    "## PPO with GAE from `stable_baseline3` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "total_t_steps = 20\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, gamma = 1.0, tensorboard_log=logdir, batch_size=2, n_steps=2)\n",
    "\n",
    "# classstable_baselines3.ppo.PPO(policy, env, learning_rate=0.0003, n_steps=2048, \n",
    "#         batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2, \n",
    "#         clip_range_vf=None, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, \n",
    "#         use_sde=False, sde_sample_freq=- 1, target_kl=None, tensorboard_log=None, \n",
    "#         create_eval_env=False, policy_kwargs=None, verbose=0, seed=None, device='auto', \n",
    "#         _init_setup_model=True)\n",
    "\n",
    "for i in range(1,10):\n",
    "    model.learn(total_timesteps=total_t_steps, reset_num_timesteps=False)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=1, eval_env=None, eval_freq=- 1, \n",
    "#       n_eval_episodes=5, tb_log_name='PPO', eval_log_path=None, reset_num_timesteps=True)\n",
    "\n",
    "    model.save(f\"{models_dir_ppo}/{total_t_steps*i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c58121",
   "metadata": {},
   "source": [
    "## From my `ddpg_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import DDPG_agent\n",
    "agent = DDPG_agent(state_size, act_size, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23bff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all rewards, energies and princip dims in files during training\n",
    "def create_info_h5(agent, env):\n",
    "    # Check if file exist and creat it\n",
    "    i = 0\n",
    "    while os.path.exists(f'run_{i}.hdf5'):\n",
    "        i += 1\n",
    "    dataFile = h5py.File(f'run_{i}.hdf5', 'a')\n",
    "    \n",
    "    # Create dataset to store info in hdf5 file\n",
    "    info = {'alg':agent.name, 'env':env.unwrapped.spec.id}\n",
    "    st = h5py.string_dtype(encoding='utf-8')\n",
    "    dataFile.create_dataset('info', dtype=st)\n",
    "    for k in info.keys():\n",
    "        dataFile['info'].attrs[k] = info[k]\n",
    "\n",
    "    # Create dataset to store hyperparams of the model in hdf5 file\n",
    "    hyperparams = {'batch_size':agent.batch_size, 'bootstrap_size':agent.bootstrap_size \\\n",
    "                   , 'gamma':agent.gamma, 'tau':agent.tau,'lr_critic':agent.lr_critic \\\n",
    "                  , 'lr_actor':agent.lr_actor, 'update_every':agent.update_every \\\n",
    "                   , 'transfer_every':agent.transfer_every, 'num_update':agent.num_update \\\n",
    "                  , 'add_noise_every':agent.add_noise_every}\n",
    "    dataFile.create_dataset('hyperparams', dtype='f')\n",
    "    for k in hyperparams.keys():\n",
    "        dataFile['hyperparams'].attrs[k] = hyperparams[k]\n",
    "    \n",
    "    # Create group for rewards, energies, princip dims, actor and critic model\n",
    "    dataFile.create_group('sigmas')\n",
    "    dataFile.create_group('rewards')\n",
    "    dataFile.create_group('energies')\n",
    "    dataFile.create_group('princip_dims')\n",
    "    dataFile.create_group('actor_models')\n",
    "    dataFile.create_group('critic_models')\n",
    "    \n",
    "    return dataFile\n",
    "\n",
    "def save_all(dat_file, i_ep, sigmas_i_ep, rew_i_ep, en_i_ep, pri_dim_i_ep, act_model_i_ep, cr_model_i_ep):\n",
    "    # Create datasets for rewards, energies, pri dim and store data in it \n",
    "    dat_file.create_dataset(f'sigmas_ep{i_ep}', dtype='f', data=sigmas_i_ep)\n",
    "    dat_file.create_dataset(f'rew_ep_{i_ep}', dtype='f', data=rew_i_ep)\n",
    "    dat_file.create_dataset(f'en_ep_{i_ep}', dtype='f', data=en_i_ep)\n",
    "    dat_file.create_dataset(f'pri_dim_ep_{i_ep}', dtype='i', data=pri_dim_i_ep)\n",
    "    \n",
    "    # Store in actor models group the network params at each ep\n",
    "    actor_model = torch.load(act_model_i_ep)\n",
    "    dat_file['actor_models'].create_dataset(f'act_mod_{i_ep}', dtype='f')\n",
    "    for k in actor_model.keys():\n",
    "        dat_file['actor_models'][f'act_mod_{i_ep}'].attrs.create(name=k,data=actor_model[k].numpy())\n",
    "    \n",
    "    # Store in actor models group the network params at each ep\n",
    "    critic_model = torch.load(cr_model_i_ep)\n",
    "    dat_file['critic_models'].create_dataset(f'cri_mod_{i_ep}', dtype='f')\n",
    "    for k in critic_model.keys():\n",
    "        dat_file['critic_models'][f'cri_mod_{i_ep}'].attrs.create(name=k,data=critic_model[k].numpy())\n",
    "\n",
    "def close_file(dat_file):\n",
    "    dat_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc90e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run ddpg algs   \n",
    "def run_ddpg(max_t_step = 10, n_episodes=10):\n",
    "    \n",
    "    # Create h5 file and store info about alg and its hypereparams\n",
    "    dat_file = create_info_h5(agent, env)\n",
    "    \n",
    "    for i_ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        rew_i_ep = []\n",
    "        en_i_ep = []\n",
    "        pri_dim_i_ep = []\n",
    "\n",
    "        ## Training loop of each episode\n",
    "        for t_step in range(max_t_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Save rew, energies, princip dims, act and crit models\n",
    "            rew_i_ep.append(reward)\n",
    "            en_i_ep.append(state[0])\n",
    "            pri_dim_i_ep.append(env.princp_dim)\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        ## Save data during training (to not lose the work done)\n",
    "        save_all(dat_file=dat_file, i_ep=int(i_ep), sigmas_i_ep=env.actions_taken \\\n",
    "                 , rew_i_ep=rew_i_ep, en_i_ep=en_i_ep, pri_dim_i_ep=pri_dim_i_ep \\\n",
    "                 , act_model_i_ep='checkpoint_actor.pth', cr_model_i_ep='checkpoint_critic.pth')\n",
    "        \n",
    "        print('Episode {} ... Score: {:.3f}'.format(i_ep, np.sum(rew_i_ep)))\n",
    "\n",
    "    close_file(dat_file)\n",
    "    return dat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051cc27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [71.902405 43.021652 61.030563]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.024218\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.1446431273843896\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [75.91998  49.982918 67.659424]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0225171\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.0263634537435244\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [54.047325 47.97421  43.36907 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.019941\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8472228415580698\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [45.917473 32.71264  44.151424]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.00938598\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1132324124482942\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [39.92289  42.064896 45.265022]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.00478516\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7932938575183357\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [42.273956 32.220497 41.48322 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00449708\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7732609288438326\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [36.666306 31.405924 48.5272  ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00441046\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7672374205066159\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [45.30961  43.95909  51.987667]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.00310242\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.24479577519942985\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [51.085533 43.112576 60.432686]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.00345636\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.22018297745209026\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [45.937973 29.533009 50.663578]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0103708\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.2606434712570369\n",
      "Episode 0 ... Score: -9.670\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [68.72811  91.20369  50.342365]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0209799\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.9194673902943258\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [ 61.029312 106.97106   55.137997]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.020393\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8786546791062353\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [ 62.891293 108.29054   60.781445]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0162137\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.5880283988738437\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [62.581646 78.9835   51.39233 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0162009\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.587138293739736\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [55.83979  82.155334 41.11377 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0109687\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2232939122806616\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [71.31832 68.53536 29.45802]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00784058\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0057661253975603\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [57.291714 58.64837  17.248169]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0142216\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.5284260376961267\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [37.524773 39.3148   32.589375]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0146475\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.5580428952442738\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [52.496433 36.739616 42.671734]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0164371\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.6824907193066494\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [67.13559  47.574066 39.55005 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0164807\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.6855226399197036\n",
      "Episode 1 ... Score: -6.748\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [48.272636 52.510765 82.15611 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0243503\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.1538431984189508\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [53.590126 37.731026 95.58353 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0232519\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.0774610515983696\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [53.445614 43.731537 80.65771 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0162515\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.5906569905980046\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [ 90.933334  29.356281 100.62697 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0155255\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.540171340022857\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [88.85588  35.762177 86.96081 ]\n",
      "Basis size (it should be the same of full dim) =   5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  0.0142177\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.449227629523989\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [52.10719  52.719715 87.21936 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00902015\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0877927904787281\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [51.45754  55.8817   89.447174]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00813429\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0261905612521414\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [55.022636 44.805836 81.474304]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00790906\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0105281878650345\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [31.286274 49.562927 81.00618 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.00698807\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9464830372821069\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [22.11605 51.79318 81.78246]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00386393\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.19184077827177148\n",
      "Episode 2 ... Score: -13.074\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [41.28518  36.45057  63.097115]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0269453\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.334298106466486\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [48.885696 52.608715 67.86743 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.024683\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.176978977959383\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [40.963646 62.518333 48.239826]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0122014\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.309015209063011\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [51.96389  59.199265 62.33108 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0121273\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.303862334810093\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [45.486855 48.869553 54.698112]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0115004\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2602680450779147\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [49.02981  44.378387 54.181858]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0112835\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2451849354226123\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [60.21576 39.43775 46.77439]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.0111167\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2335857528937773\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [52.481426 49.636017 33.13845 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00229314\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.6200001234325487\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [57.314068 46.78891  39.67016 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.000170861\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.4486548155774326\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [54.418877 45.386677 52.971207]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00236164\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.29630921904661633\n",
      "Episode 3 ... Score: -12.228\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [52.467445 51.71169  71.5673  ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0249694\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.1968950803350324\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [81.74588 60.88283 64.59455]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0210752\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.9260945011756085\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [63.126877 57.831738 42.65856 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0118924\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2875275148099519\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [38.496803 41.43422  50.626236]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0118915\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2874649292927103\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [49.133778 41.756874 70.89046 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0116566\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2711301092925673\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [35.424072 34.93599  53.80532 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00631068\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8993777000326588\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [51.10303  21.357079 54.4025  ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.00449941\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.14764983994188619\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [46.227783 29.862051 47.2418  ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0137251\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.4938996940176654\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [21.547539 32.911674 42.883266]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0222251\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.0849851346357884\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [26.889235 28.429327 37.282043]\n",
      "Basis size (it should be the same of full dim) =   10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  -0.0248619\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.268346792261891\n",
      "Episode 4 ... Score: -6.169\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [53.21033  38.890873 79.68896 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0244839\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.1631336707561974\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [48.087555 34.47763  85.89944 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0239656\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.1270913667712126\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [49.93962  32.650818 72.704315]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0201472\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8615618789528323\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [40.58517  24.453402 82.77689 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  -0.00172111\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.3408513316676913\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [42.18734 30.76767 78.51843]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.00255074\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.28315930647945464\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [49.692432 28.664423 93.4601  ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.00640208\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.01533918873001916\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [51.952686 22.763287 88.26336 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0064321\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.013251614032682824\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [ 62.384598  30.943026 100.764725]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0100139\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.23582483669743404\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [49.54054  26.451126 82.55838 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0139549\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.5098798627534364\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [59.634155 52.890392 90.94258 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0172302\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.7376424678894988\n",
      "Episode 5 ... Score: -5.321\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [51.805183 41.97865  64.49507 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0264004\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.296406052749683\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [61.972908 56.325157 55.29589 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.025157\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2099406837067903\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [40.926174 45.91939  65.499916]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0192352\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.7981418881476863\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [33.297363 41.76464  77.57374 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0100929\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1623912500579134\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [16.316666 54.713074 90.01883 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.00974453\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.21709299138692728\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [20.218277 66.528854 86.0924  ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.0154166\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.6115256967006726\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [10.439186 63.486572 98.50937 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0220599\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.0734972152487163\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [ 24.36499  45.51314 109.23511]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0226239\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.1127174727203197\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [ 25.171078  35.607635 105.2433  ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.026138\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.3570861017645708\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [17.896374 36.429688 95.62415 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0426205\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.5032703105867142\n",
      "Episode 6 ... Score: -0.592\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [49.669075 47.010593 61.095184]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0269296\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.3332063368879314\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [41.052017 54.98289  77.617874]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0208976\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.913744292439869\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [32.331787 50.438408 72.905624]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.016757\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.6258091894488835\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [28.48345 37.70308 70.37085]\n",
      "Basis size (it should be the same of full dim) =   4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  0.00872258\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0670999322946422\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [27.53262  34.254066 66.865654]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.00315219\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.6797379996399613\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [27.293844 38.447002 71.40506 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.00396021\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.1851455187161566\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [20.830357 38.97498  71.64119 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0222771\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.0886011867430998\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [18.16933  48.305332 77.15411 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0222775\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.088629002528542\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [15.080254 32.620567 86.62988 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0503768\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  3.04263925212393\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [29.18683  31.222744 76.52591 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0518241\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  3.1432837177952955\n",
      "Episode 7 ... Score: 0.558\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [43.64206  28.043348 74.22672 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0205854\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8920340719032858\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [30.61948  33.439507 91.51152 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0189796\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.7803676012509797\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [36.446365 35.909546 82.232475]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0167899\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.6280970378013926\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [55.609955  8.381809 91.15785 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  -0.0202134\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.9450925957073792\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [68.14066  19.290443 94.09574 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.0218034\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.05566034283477\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [48.209488 24.344898 84.0375  ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.0242034\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.2225550554798854\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [54.973133 22.983482 70.171906]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0262231\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.3630039101171132\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [29.032635 21.521473 68.170135]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0421613\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  2.4713377889006143\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [18.496147 20.052643 88.44583 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0638376\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  3.978696063779509\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [48.168327  0.       74.51254 ]\n",
      "**** ILLEGAL ACTION **** --> Set reward: -10.0\n",
      "This action IS REMOVED from actions taken and sigmas, the energy is NOT STORED!\n",
      "Episode 8 ... Score: -4.264\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [21.849354 13.383713 89.25873 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  -0.061313\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  3.803136733969567\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [ 31.256598  31.045422 106.22699 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  -0.064273\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  4.008973546231878\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [37.04305  20.71502  97.663025]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  -0.0693844\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  4.364417560487816\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [29.56744  15.539604 79.87933 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  -0.0787808\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  5.017838176278889\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [27.60569  19.639462 83.72986 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.0820079\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  5.24224897926933\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [24.75789  16.849003 83.22301 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.0963054\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  6.236489460120811\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [26.665401 31.565578 76.25976 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0972211\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  6.300166746941285\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [13.204262 68.823494 88.685455]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.097689\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  6.332704261960723\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [15.710667 61.403034 81.679794]\n",
      "Basis size (it should be the same of full dim) =   9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  -0.0979385\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  6.350054358129453\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [21.708878 54.274143 82.9246  ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.098064\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  6.358781560811521\n",
      "Episode 9 ... Score: 54.015\n"
     ]
    }
   ],
   "source": [
    "all_data = run_ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = np.loadtxt('scores_RL.out')\n",
    "energies = np.loadtxt('energies_RL.out')\n",
    "dim = np.loadtxt('princip_dim_RL.out')\n",
    "\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(energies)), energies)\n",
    "plt.ylabel('Eenergies (mK)')\n",
    "plt.xlabel('Episode #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(dim)), dim)\n",
    "plt.ylabel('dim (mK)')\n",
    "plt.xlabel('Episode #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7337599",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for i in range(10):\n",
    "    rew_i_episode = []\n",
    "    for j in range(20):\n",
    "        rew_i_episode.append(j)\n",
    "    rewards.append(rew_i_episode)\n",
    "    print('Episode {} ... Score: {:.3f}'.format(i, np.sum(rewards[i])))\n",
    "    \n",
    "name_rewards = 'rewards_RL.out'\n",
    "file_rewards = open(name_rewards,'w')\n",
    "np.savetxt(file_rewards, rewards, fmt=\"%f\")\n",
    "file_rewards.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d72c",
   "metadata": {},
   "source": [
    "## Random search as in original SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "scores = []\n",
    "step = 0\n",
    "score = 0.0\n",
    "\n",
    "while True:\n",
    "    print(\".....STEP.....\", step)\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    step = step + 1\n",
    "    score += reward\n",
    "    scores.append(score)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
