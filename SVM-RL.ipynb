{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8994df9d",
   "metadata": {},
   "source": [
    "# Stochastic Variational Method with RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import svm_env as svm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8829f8",
   "metadata": {},
   "source": [
    "## Expoloring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('svm_env:svmEnv-v0', file_sigmas =\"./svmCodeSVD/sigmas.dat\" )\n",
    "\n",
    "obs_space = env.observation_space\n",
    "\n",
    "print('###### Observation space ####### \\n', obs_space)\n",
    "\n",
    "state_size = env.observation_space.shape[-1]\n",
    "\n",
    "print('###### Size of observation space ####### \\n', state_size)\n",
    "\n",
    "act_space = env.action_space\n",
    "\n",
    "print('###### Action space ####### \\n', act_space)\n",
    "\n",
    "act_size = env.action_space.shape[-1]\n",
    "\n",
    "print('###### Number of actions ####### \\n', act_size)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "print('##### State after reset ###### \\n', state)\n",
    "\n",
    "print('##### File where will be stored sigmas \\n', env.file_sigmas)\n",
    "\n",
    "act_space.sample()\n",
    "\n",
    "env.princp_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927566a",
   "metadata": {},
   "source": [
    "## DDPG from `stable_baseline3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for DDPG\n",
    "action_noise = NormalActionNoise(mean=np.zeros(act_size), sigma=0.1 * np.ones(act_size))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise = action_noise, \\\n",
    "            batch_size=128, gamma=1.0, verbose=1)\n",
    "\n",
    "# (policy, env, learning_rate=0.001, buffer_size=1000000,learning_starts=100, batch_size=100, \n",
    "# tau=0.005, gamma=0.99, train_freq=(1, 'episode'),  gradient_steps=- 1, action_noise=None, \n",
    "# replay_buffer_class=None, replay_buffer_kwargs=None,  optimize_memory_usage=False, \n",
    "# tensorboard_log=None, create_eval_env=False, policy_kwargs=None,  verbose=0, seed=None, \n",
    "# device='auto', _init_setup_model=True)\n",
    "\n",
    "model.learn(total_timesteps=1000, log_interval=5)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=4, eval_env=None, eval_freq=- 1,\n",
    "# n_eval_episodes=5, tb_log_name='DDPG', eval_log_path=None, reset_num_timesteps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c58121",
   "metadata": {},
   "source": [
    "## From my `ddpg_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc90e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('svm_env:svmEnv-v0')\n",
    "# Instance of the ddpg agent\n",
    "agent = Agent(1, 3, random_seed=2)\n",
    "\n",
    "### Training loop\n",
    "def run_ddpg(max_t_step = 300, n_episodes = 3):        \n",
    "    \"\"\"Deep Deterministic Policy Gradient learning for Reacher Unity Environment.\n",
    "    \n",
    "    Params Input\n",
    "    ==========\n",
    "        n_episode (int): maximum number of episodes\n",
    "        queue (int): number of consecutive episodes \n",
    "        \n",
    "    Params Output\n",
    "    ==========\n",
    "        scores_all (list of floats): are the scores collected at the end of each episode\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ##Inizialization\n",
    "    scores = []                         \n",
    "    last_energies = []\n",
    "    princip_dim = []    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        agent.reset()                  \n",
    "        score = 0.0\n",
    "        \n",
    "        ## Training loop of each episode\n",
    "        for t_step in range(max_t_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)                   \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward\n",
    "            state = next_state  \n",
    "            if done:                                  \n",
    "                break\n",
    "        \n",
    "        scores.append(score)\n",
    "        last_energies.append(state[0])\n",
    "        princip_dim.append(agent.princp_dim)\n",
    "        \n",
    "        print('Episode {} ... Reward: {:.3f}'.format(i_episode, score))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = run_ddpg()\n",
    "torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d72c",
   "metadata": {},
   "source": [
    "## Random search as in original SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "scores = []\n",
    "step = 0\n",
    "score = 0.0\n",
    "\n",
    "while True:\n",
    "    print(\".....STEP.....\", step)\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    step = step + 1\n",
    "    score += reward\n",
    "    scores.append(score)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f05704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
