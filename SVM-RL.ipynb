{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8994df9d",
   "metadata": {},
   "source": [
    "# Stochastic Variational Method with RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a46171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8829f8",
   "metadata": {},
   "source": [
    "## Expoloring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3a490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Observation space ####### \n",
      " Box(-inf, inf, (1,), float32)\n",
      "###### Size of observation space ####### \n",
      " 1\n",
      "###### Action space ####### \n",
      " Box(-1.0, 1.0, (3,), float32)\n",
      "###### Number of actions ####### \n",
      " 3\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "##### State after reset ###### \n",
      " [0.]\n",
      "##### File where will be stored sigmas \n",
      " ./svmCodeSVD/sigmas1.dat\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('svm_env:svmEnv-v1', file_sigmas =\"./svmCodeSVD/sigmas1.dat\" )\n",
    "\n",
    "obs_space = env.observation_space\n",
    "\n",
    "print('###### Observation space ####### \\n', obs_space)\n",
    "\n",
    "state_size = env.observation_space.shape[-1]\n",
    "\n",
    "print('###### Size of observation space ####### \\n', state_size)\n",
    "\n",
    "act_space = env.action_space\n",
    "\n",
    "print('###### Action space ####### \\n', act_space)\n",
    "\n",
    "act_size = env.action_space.shape[-1]\n",
    "\n",
    "print('###### Number of actions ####### \\n', act_size)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "print('##### State after reset ###### \\n', state)\n",
    "\n",
    "print('##### File where will be stored sigmas \\n', env.file_sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927566a",
   "metadata": {},
   "source": [
    "## Twin Delayed DDPG (TD3) from `stable_baseline3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for DDPG\n",
    "action_noise = NormalActionNoise(mean=np.zeros(act_size), sigma=0.2 * np.ones(act_size))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise = action_noise, batch_size=64, gamma=1.0, verbose=1, seed = 2)\n",
    "\n",
    "# (policy, env, learning_rate=0.001, buffer_size=1000000,learning_starts=100, batch_size=100, \n",
    "# tau=0.005, gamma=0.99, train_freq=(1, 'episode'),  gradient_steps=- 1, action_noise=None, \n",
    "# replay_buffer_class=None, replay_buffer_kwargs=None,  optimize_memory_usage=False, \n",
    "# tensorboard_log=None, create_eval_env=False, policy_kwargs=None,  verbose=0, seed=None, \n",
    "# device='auto', _init_setup_model=True)\n",
    "\n",
    "model.learn(total_timesteps=300, log_interval = 5, n_eval_episodes = 1)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=4, eval_env=None, eval_freq=- 1,\n",
    "# n_eval_episodes=5, tb_log_name='DDPG', eval_log_path=None, reset_num_timesteps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4411e58",
   "metadata": {},
   "source": [
    "## PPO with GAE from `stable_baseline3` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, n_steps = 2, batch_size = 64, gamma = 1.0)\n",
    "\n",
    "# classstable_baselines3.ppo.PPO(policy, env, learning_rate=0.0003, n_steps=2048, \n",
    "#         batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2, \n",
    "#         clip_range_vf=None, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, \n",
    "#         use_sde=False, sde_sample_freq=- 1, target_kl=None, tensorboard_log=None, \n",
    "#         create_eval_env=False, policy_kwargs=None, verbose=0, seed=None, device='auto', \n",
    "#         _init_setup_model=True)\n",
    "\n",
    "model.learn(total_timesteps = 300, n_eval_episodes = 1)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=1, eval_env=None, eval_freq=- 1, \n",
    "#       n_eval_episodes=5, tb_log_name='PPO', eval_log_path=None, reset_num_timesteps=True)\n",
    "\n",
    "model.save(\"ppo_svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_svm\")\n",
    "\n",
    "obs = env.reset()\n",
    "rewards = []\n",
    "score = 0.0\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c58121",
   "metadata": {},
   "source": [
    "## From my `ddpg_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent_bootstrap import DDPG_agent, ActionNoise, ParameterNoise, ReplayBuffer, OUNoise\n",
    "agent = DDPG_agent(state_size, act_size, device = device, seed = 0)\n",
    "noise = ActionNoise(act_size, device = device, seed = 0)\n",
    "agent.set_noise(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc90e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all rewards, energies and princip dims in files during training\n",
    "\n",
    "def save_all(agent, rewards, energies, princip_dims):\n",
    "    torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "    torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "\n",
    "    name_rewards = 'rewards_RL_0.out'\n",
    "    file_rewards = open(name_rewards,'w')\n",
    "    np.savetxt(file_rewards, rewards, fmt=\"%f\")\n",
    "    file_rewards.close()\n",
    "\n",
    "    name_energies = 'energies_RL_0.out'\n",
    "    file_energies = open(name_energies,'w')\n",
    "    np.savetxt(file_energies, energies, fmt=\"%f\")\n",
    "    file_energies.close()\n",
    "\n",
    "    name_dim = 'princip_dims_RL_0.out'\n",
    "    file_dim = open(name_dim,'w')\n",
    "    np.savetxt(file_dim, princip_dims, fmt=\"%f\")\n",
    "    file_dim.close()\n",
    "\n",
    "def run_ddpg(max_t_step = 300, n_episodes=700):\n",
    "    ##Inizialization\n",
    "    rewards = []\n",
    "    energies = []  \n",
    "    princip_dims = []\n",
    "\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        rew_per_i_episode = []\n",
    "        energies_per_i_episode = []\n",
    "        princip_dim_per_i_episode = []\n",
    "\n",
    "        ## Training loop of each episode\n",
    "        for t_step in range(max_t_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Save\n",
    "            rew_per_i_episode.append(reward)\n",
    "            energies_per_i_episode.append(state[0])\n",
    "            princip_dim_per_i_episode.append(env.princp_dim)\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        ## Save data during training (to not lose the work done)\n",
    "        rewards.append(rew_per_i_episode)\n",
    "        energies.append(energies_per_i_episode)\n",
    "        princip_dims.append(princip_dim_per_i_episode)\n",
    "\n",
    "        save_all(agent, rewards, energies, princip_dims)\n",
    "\n",
    "        print('Episode {} ... Score: {:.3f}'.format(i_episode, np.sum(rewards[i_episode])))\n",
    "\n",
    "        \n",
    "    return rewards, energies, princip_dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051cc27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1abf78269131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_energies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_princip_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-44fd5a2c6d73>\u001b[0m in \u001b[0;36mrun_ddpg\u001b[0;34m(max_t_step, n_episodes)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m## Training loop of each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PhD_Nice/svm-reinforce/ddpg_agent_bootstrap.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, add_noise)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mto_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PhD_Nice/svm-reinforce/ddpg_agent_bootstrap.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, model, state)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "all_rewards, all_energies, all_princip_dim = run_ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = np.loadtxt('scores_RL.out')\n",
    "energies = np.loadtxt('energies_RL.out')\n",
    "dim = np.loadtxt('princip_dim_RL.out')\n",
    "\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(energies)), energies)\n",
    "plt.ylabel('Eenergies (mK)')\n",
    "plt.xlabel('Episode #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(dim)), dim)\n",
    "plt.ylabel('dim (mK)')\n",
    "plt.xlabel('Episode #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7337599",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for i in range(10):\n",
    "    rew_i_episode = []\n",
    "    for j in range(20):\n",
    "        rew_i_episode.append(j)\n",
    "    rewards.append(rew_i_episode)\n",
    "    print('Episode {} ... Score: {:.3f}'.format(i, np.sum(rewards[i])))\n",
    "    \n",
    "name_rewards = 'rewards_RL.out'\n",
    "file_rewards = open(name_rewards,'w')\n",
    "np.savetxt(file_rewards, rewards, fmt=\"%f\")\n",
    "file_rewards.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d72c",
   "metadata": {},
   "source": [
    "## Random search as in original SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "scores = []\n",
    "step = 0\n",
    "score = 0.0\n",
    "\n",
    "while True:\n",
    "    print(\".....STEP.....\", step)\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    step = step + 1\n",
    "    score += reward\n",
    "    scores.append(score)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
