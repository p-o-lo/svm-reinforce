{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8994df9d",
   "metadata": {},
   "source": [
    "# Stochastic Variational Method with RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a46171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import torch\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8829f8",
   "metadata": {},
   "source": [
    "## Expoloring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f3a490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Env Name ###### svmEnv-v1\n",
      "###### Observation space ####### \n",
      " Box(-inf, inf, (1,), float32)\n",
      "###### Size of observation space ####### \n",
      " 1\n",
      "###### Action space ####### \n",
      " Box(-1.0, 1.0, (3,), float32)\n",
      "###### Number of actions ####### \n",
      " 3\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "##### State after reset ###### \n",
      " [0.]\n",
      "##### File where will be stored sigmas \n",
      " ./svmCodeSVD/sigmas.dat\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('svm_env:svmEnv-v1', file_sigmas =\"./svmCodeSVD/sigmas.dat\" )\n",
    "\n",
    "print('### Env Name ######', env.unwrapped.spec.id)\n",
    "\n",
    "obs_space = env.observation_space\n",
    "\n",
    "print('###### Observation space ####### \\n', obs_space)\n",
    "\n",
    "state_size = env.observation_space.shape[-1]\n",
    "\n",
    "print('###### Size of observation space ####### \\n', state_size)\n",
    "\n",
    "act_space = env.action_space\n",
    "\n",
    "print('###### Action space ####### \\n', act_space)\n",
    "\n",
    "act_size = env.action_space.shape[-1]\n",
    "\n",
    "print('###### Number of actions ####### \\n', act_size)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "print('##### State after reset ###### \\n', state)\n",
    "\n",
    "print('##### File where will be stored sigmas \\n', env.file_sigmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6da707",
   "metadata": {},
   "source": [
    "## Saving folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40786186",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir_ppo = 'models/PPO'\n",
    "models_dir_td3 = 'models/TD3'\n",
    "\n",
    "logdir = 'logs'\n",
    "\n",
    "if not os.path.exists(models_dir_ppo):\n",
    "    os.makedirs(models_dir_ppo)\n",
    "    \n",
    "if not os.path.exists(models_dir_td3):\n",
    "    os.makedirs(models_dir_td3)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927566a",
   "metadata": {},
   "source": [
    "## Twin Delayed DDPG (TD3) from `stable_baseline3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for DDPG\n",
    "action_noise = NormalActionNoise(mean=np.zeros(act_size), sigma=0.2 * np.ones(act_size))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, batch_size=64, gamma=1.0, verbose=1, seed=0\n",
    "            , tensorboard_log=logdir )\n",
    "\n",
    "# (policy, env, learning_rate=0.001, buffer_size=1000000,learning_starts=100, batch_size=100, \n",
    "# tau=0.005, gamma=0.99, train_freq=(1, 'episode'),  gradient_steps=- 1, action_noise=None, \n",
    "# replay_buffer_class=None, replay_buffer_kwargs=None,  optimize_memory_usage=False, \n",
    "# tensorboard_log=None, create_eval_env=False, policy_kwargs=None,  verbose=0, seed=None, \n",
    "# device='auto', _init_setup_model=True)\n",
    "\n",
    "model.learn(total_timesteps=300, log_interval = 5, n_eval_episodes = 1)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=4, eval_env=None, eval_freq=- 1,\n",
    "# n_eval_episodes=5, tb_log_name='DDPG', eval_log_path=None, reset_num_timesteps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4411e58",
   "metadata": {},
   "source": [
    "## PPO with GAE from `stable_baseline3` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "total_t_steps = 20\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, gamma = 1.0, tensorboard_log=logdir, batch_size=2, n_steps=2)\n",
    "\n",
    "# classstable_baselines3.ppo.PPO(policy, env, learning_rate=0.0003, n_steps=2048, \n",
    "#         batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2, \n",
    "#         clip_range_vf=None, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, \n",
    "#         use_sde=False, sde_sample_freq=- 1, target_kl=None, tensorboard_log=None, \n",
    "#         create_eval_env=False, policy_kwargs=None, verbose=0, seed=None, device='auto', \n",
    "#         _init_setup_model=True)\n",
    "\n",
    "for i in range(1,10):\n",
    "    model.learn(total_timesteps=total_t_steps, reset_num_timesteps=False)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=1, eval_env=None, eval_freq=- 1, \n",
    "#       n_eval_episodes=5, tb_log_name='PPO', eval_log_path=None, reset_num_timesteps=True)\n",
    "\n",
    "    model.save(f\"{models_dir_ppo}/{total_t_steps*i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c58121",
   "metadata": {},
   "source": [
    "## From my `ddpg_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import DDPG_agent\n",
    "agent = DDPG_agent(state_size, act_size, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eda0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all rewards, energies and princip dims in files during training\n",
    "def create_info_h5(agent, env):\n",
    "    # Check if file exist and creat it\n",
    "    i = 0\n",
    "    while os.path.exists(f'run_{i}.hdf5'):\n",
    "        i += 1\n",
    "    dataFile = h5py.File(f'run_{i}.hdf5', 'a')\n",
    "    \n",
    "    # Create dataset to store info in hdf5 file\n",
    "    info = {'alg':agent.name, 'env':env.unwrapped.spec.id}\n",
    "    st = h5py.string_dtype(encoding='utf-8')\n",
    "    dataFile.create_dataset('info', dtype=st)\n",
    "    for k in info.keys():\n",
    "        dataFile['info'].attrs[k] = info[k]\n",
    "\n",
    "    # Create dataset to store hyperparams of the model in hdf5 file\n",
    "    hyperparams = {'batch_size':agent.batch_size, 'bootstrap_size':agent.bootstrap_size \\\n",
    "                   , 'gamma':agent.gamma, 'tau':agent.tau,'lr_critic':agent.lr_critic \\\n",
    "                  , 'lr_actor':agent.lr_actor, 'update_every':agent.update_every \\\n",
    "                   , 'transfer_every':agent.transfer_every, 'num_update':agent.num_update \\\n",
    "                  , 'add_noise_every':agent.add_noise_every}\n",
    "    dataFile.create_dataset('hyperparams', dtype='f')\n",
    "    for k in hyperparams.keys():\n",
    "        dataFile['hyperparams'].attrs[k] = hyperparams[k]\n",
    "    \n",
    "    # Create group for rewards, energies, princip dims, actor and critic model\n",
    "    dataFile.create_group('sigmas')\n",
    "    dataFile.create_group('rewards')\n",
    "    dataFile.create_group('energies')\n",
    "    dataFile.create_group('princip_dims')\n",
    "    dataFile.create_group('actor_models')\n",
    "    dataFile.create_group('critic_models')\n",
    "    \n",
    "    return dataFile\n",
    "\n",
    "def save_all(dat_file, i_ep, sigmas_i_ep, rew_i_ep, en_i_ep, pri_dim_i_ep, act_model_i_ep, cr_model_i_ep):\n",
    "    # Create datasets for rewards, energies, pri dim and store data in it \n",
    "    dat_file['sigmas'].create_dataset(f'sigmas_ep_{i_ep}', dtype='f', data=sigmas_i_ep)\n",
    "    dat_file['rewards'].create_dataset(f'rew_ep_{i_ep}', dtype='f', data=rew_i_ep)\n",
    "    dat_file['energies'].create_dataset(f'en_ep_{i_ep}', dtype='f', data=en_i_ep)\n",
    "    dat_file['princip_dims'].create_dataset(f'pri_dim_ep_{i_ep}', dtype='i', data=pri_dim_i_ep)\n",
    "    \n",
    "    # Store in actor models group the network params at each ep\n",
    "    actor_model = torch.load(act_model_i_ep)\n",
    "    dat_file['actor_models'].create_dataset(f'act_mod_{i_ep}', dtype='f')\n",
    "    for k in actor_model.keys():\n",
    "        dat_file['actor_models'][f'act_mod_{i_ep}'].attrs.create(name=k,data=actor_model[k].numpy())\n",
    "    \n",
    "    # Store in actor models group the network params at each ep\n",
    "    critic_model = torch.load(cr_model_i_ep)\n",
    "    dat_file['critic_models'].create_dataset(f'cri_mod_{i_ep}', dtype='f')\n",
    "    for k in critic_model.keys():\n",
    "        dat_file['critic_models'][f'cri_mod_{i_ep}'].attrs.create(name=k,data=critic_model[k].numpy())\n",
    "\n",
    "def close_file(dat_file, actor_model_file, critic_model_file):\n",
    "    dat_file.close()\n",
    "    os.remove(actor_model_file)\n",
    "    os.remove(critic_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc90e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run ddpg algs   \n",
    "def run_ddpg(max_t_step = 10, n_episodes=10):\n",
    "    \n",
    "    # Create h5 file and store info about alg and its hypereparams\n",
    "    dat_file = create_info_h5(agent, env)\n",
    "    \n",
    "    for i_ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        rew_i_ep = []\n",
    "        en_i_ep = []\n",
    "        pri_dim_i_ep = []\n",
    "\n",
    "        ## Training loop of each episode\n",
    "        for t_step in range(max_t_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Save rew, energies, princip dims, act and crit models\n",
    "            rew_i_ep.append(reward)\n",
    "            en_i_ep.append(state[0])\n",
    "            pri_dim_i_ep.append(env.princp_dim)\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        ## Save data during training (to not lose the work done)\n",
    "        save_all(dat_file=dat_file, i_ep=int(i_ep), sigmas_i_ep=env.actions_taken \\\n",
    "                 , rew_i_ep=rew_i_ep, en_i_ep=en_i_ep, pri_dim_i_ep=pri_dim_i_ep \\\n",
    "                 , act_model_i_ep='checkpoint_actor.pth', cr_model_i_ep='checkpoint_critic.pth')\n",
    "        \n",
    "        print('Episode {} ... Score: {:.3f}'.format(i_ep, np.sum(rew_i_ep)))\n",
    "\n",
    "    close_file(dat_file, 'checkpoint_actor.pth', 'checkpoint_critic.pth')\n",
    "    return dat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051cc27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [53.599854 52.168858 55.192825]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0267298\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.3193123520602263\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [47.9587   46.173054 58.668888]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0267081\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.31780334570006\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [40.855743 40.66587  55.741226]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0144049\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.4622454171103083\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [52.3661   27.6421   73.346855]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.00708029\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.952895966615495\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [53.752167 22.827332 48.90146 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.005674\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.06596948138945713\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [38.26042  38.85783  48.200912]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.00567473\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.06591871758103096\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [18.904968 41.801132 49.591633]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0325822\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.8052123131094309\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [31.873383 30.85493  53.628506]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.033252\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.8517898458301385\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [39.712082 30.49872  43.14158 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0333648\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.8596338973244588\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [39.19693  29.646362 50.321587]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0334395\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.864828495255539\n",
      "Episode 0 ... Score: 0.197\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [56.29423  58.119614 66.19611 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0243228\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.151930863169893\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [65.73262 52.14472 70.68179]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.020647\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8963177028611753\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [51.774788 65.211685 51.92993 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0150864\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.509636561555162\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [45.858532 68.50077  48.561745]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0134805\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.3979631369564967\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [45.250687 69.46339  50.005047]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0133061\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.385835454504285\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [40.100502 50.59441  50.062546]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0115849\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2661441297522966\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [63.91501  36.97354  52.753403]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00948852\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1203629890460576\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [63.711544 27.47895  30.790916]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.000436528\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.49089232083526646\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [55.33873  31.440979 37.960022]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.00489944\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.11983196831712561\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [62.97274  38.236797 40.735653]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00642337\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.013858693549927636\n",
      "Episode 1 ... Score: -11.353\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [56.93234  36.349075 49.710625]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0267057\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.3176364509874148\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [56.21603  41.980625 38.69761 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.025488\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2329582461590984\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [74.27842  43.412033 29.339321]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0147738\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.4878985252331347\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [64.32888  65.264114 46.704998]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.00690743\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9408753749372298\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [70.783356 60.942764 45.078667]\n",
      "Basis size (it should be the same of full dim) =   5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  0.00668968\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9257331567378682\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [79.343376 55.424946 54.08773 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00661243\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9203612331746029\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [83.002594 44.267334 43.764   ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00598362\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8766341230669443\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [79.8156   35.43865  47.766567]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00171445\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.5797583312406314\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [71.468636 25.712475 55.91379 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.000424603\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.4900630627318119\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [55.87331  30.8623   49.368725]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00106393\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.3865512763577392\n",
      "Episode 2 ... Score: -11.158\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [63.91895 61.05257 82.33938]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0212399\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.937547650830881\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [76.47029  57.024605 82.89536 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0172405\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.6594315201005134\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [73.26312 55.32296 70.4788 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0139493\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.4305632374931765\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [93.328125 75.638115 68.416306]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.011137\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2349974040049023\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [95.10968 79.31593 58.33383]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0111246\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.234135114656235\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [102.89708   83.49572   63.747093]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00951151\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1219617013142695\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [82.52501  99.980576 67.33791 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00950557\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1215486369004726\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [67.5931  89.74011 64.95271]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00882212\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0740218905015961\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [63.00254  77.262276 64.74771 ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.00882208\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0740191089230535\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [56.817352 94.655396 76.5735  ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  0.00876858\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.07029874762034\n",
      "Episode 3 ... Score: -12.959\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [43.354336 28.590498 48.75914 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0226908\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.0384424585712146\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [45.331974 36.593277 42.799774]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0206719\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8980492355048693\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [36.641083 50.068172 44.97287 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.00696358\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9447800158184894\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [46.333694 40.03544  53.201263]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.00110334\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.5372620696387322\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [38.485218 51.760075 65.95332 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.000381753\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.48708329671646133\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [47.78264  55.782658 59.7884  ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.000133399\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.4698128927730174\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [46.564194 63.253258 58.89809 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.00582987\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.05513036519779568\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [37.628635 66.800385 51.67844 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.00635516\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.01860198036223082\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [40.15927  64.68184  57.444485]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.00688191\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.018027932090193488\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [53.538456 44.010864 72.476166]\n",
      "Basis size (it should be the same of full dim) =   10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  -0.00705983\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.030400393454282693\n",
      "Episode 4 ... Score: -6.401\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [53.652485 38.57302  50.18141 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0275277\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.374797890068365\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [56.059307 33.49945  71.20169 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0193713\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8076062091439375\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [60.226246 20.773403 70.867226]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.00762291\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9906294703552856\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [74.038445 31.090687 58.2015  ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  -0.00113584\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.3815506935301105\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [87.7323   53.87647  63.460396]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.00295145\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.25529414801944483\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [67.591125 56.79714  61.308292]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.00457806\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.14218056112957989\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [68.41962 54.52864 61.51589]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.00461371\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.1397014792521638\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [34.828106 58.103615 80.13921 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.00639119\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.016096473488646623\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [36.32856 73.25478 81.25227]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.00698598\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.025264904067265448\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [32.956295 61.966927 70.14636 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00699829\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.026120934864207968\n",
      "Episode 5 ... Score: -6.056\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [73.10079 70.67363 60.74149]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0213987\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.9485905176508993\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [77.07216  73.60462  50.258446]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0213985\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.9485766097581774\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [79.925285 66.39299  60.365513]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0173865\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.669584281786424\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [88.351135 62.175102 46.90764 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0170004\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.642735094889641\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [103.65607   63.509804  51.772   ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0124415\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.3257116342738815\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [110.       70.12919  66.54469]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0104672\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.188419871284193\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [100.70291  83.9219   77.16011]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.010217\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1710210974909394\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [89.94821 69.34823 87.82564]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.010057\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1598947833145985\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [84.7216   83.78662  97.193565]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.00923032\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.102407899543989\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [92.94287 69.09335 93.28395]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  0.00829731\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0375268846085621\n",
      "Episode 6 ... Score: -14.194\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [54.668694 44.7543   82.190704]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0242517\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.146986607307781\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [48.105038 59.87108  83.03877 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0228523\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.0496730819429576\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [44.522873 46.546883 76.033875]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0198052\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.8377793824009014\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [40.274334 49.344036 60.885277]\n",
      "Basis size (it should be the same of full dim) =   4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  0.0137308\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.4153688646961093\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [24.308012 68.69279  63.10993 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.00386346\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7291993339162506\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [26.528606 78.25482  71.71178 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00167807\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.5772284855547856\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [30.382992 61.058044 63.960793]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.000727558\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.511130390927395\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [40.22537  63.14935  78.401245]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.00292962\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.25681219450987847\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [49.836983 85.34392  75.026596]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0029395\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.2561251446094879\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [50.430935 87.03938  68.63148 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00336028\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.22686432911498322\n",
      "Episode 7 ... Score: -10.007\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [52.75957 69.20112 50.96772]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0252735\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2180420312164415\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [54.08663  71.848976 61.16136 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0230966\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.0666615729009585\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [51.05004  68.55985  44.658287]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0126525\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.340384461093933\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [62.282574 68.46456  36.191566]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0117888\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2803232263807693\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [64.68407  76.4539   44.066406]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0108814\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2172231171081958\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [72.66188  77.58348  55.686226]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0089813\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0850911823177842\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [44.743633 72.82198  48.633823]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00891639\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0805773757353698\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [54.201977 76.18064  49.979168]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00891491\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0804744573292382\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [58.56063  69.03496  24.478481]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.000884814\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.39900690692029706\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [52.407063 51.02818  24.263218]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00275253\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.26912693811917876\n",
      "Episode 8 ... Score: -12.037\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [51.905098 41.789036 77.697266]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0250936\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2055318817144176\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [47.866947 18.063591 80.72664 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  -0.00421363\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.16752282785010486\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [59.523575 18.933727 96.51474 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  -0.00909878\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.17218788276585073\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [55.141685 36.923645 89.658554]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  -0.00909913\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.17221222157811233\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [ 53.093777  40.10986  105.76463 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  -0.0126842\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.4215160663542079\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [ 65.78      18.825054 104.38747 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  -0.0209444\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.9959259436005379\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [70.653366 30.296743 88.80952 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.021084\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.0056336527193963\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [60.122017 32.369354 85.36804 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0211323\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.0089924088113786\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [64.65029  22.988728 73.74574 ]\n",
      "Basis size (it should be the same of full dim) =   9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  -0.0233335\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.1620626760923933\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [ 54.418785  46.364475 103.415085]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.0246896\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.2563651426832436\n",
      "Episode 9 ... Score: 3.822\n"
     ]
    }
   ],
   "source": [
    "all_data = run_ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d72c",
   "metadata": {},
   "source": [
    "## Random search as in original SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "scores = []\n",
    "step = 0\n",
    "score = 0.0\n",
    "\n",
    "while True:\n",
    "    print(\".....STEP.....\", step)\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    step = step + 1\n",
    "    score += reward\n",
    "    scores.append(score)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
