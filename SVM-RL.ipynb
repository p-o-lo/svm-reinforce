{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8994df9d",
   "metadata": {},
   "source": [
    "# Stochastic Variational Method with RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a46171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8829f8",
   "metadata": {},
   "source": [
    "## Expoloring environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f3a490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Env Name ###### svmEnv-v1\n",
      "###### Observation space ####### \n",
      " Box(-inf, inf, (1,), float32)\n",
      "###### Size of observation space ####### \n",
      " 1\n",
      "###### Action space ####### \n",
      " Box(-1.0, 1.0, (3,), float32)\n",
      "###### Number of actions ####### \n",
      " 3\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "##### State after reset ###### \n",
      " [0.]\n",
      "##### File where will be stored sigmas \n",
      " ./svmCodeSVD/sigmas.dat\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('svm_env:svmEnv-v1', file_sigmas =\"./svmCodeSVD/sigmas.dat\" )\n",
    "\n",
    "print('### Env Name ######', env.unwrapped.spec.id)\n",
    "\n",
    "obs_space = env.observation_space\n",
    "\n",
    "print('###### Observation space ####### \\n', obs_space)\n",
    "\n",
    "state_size = env.observation_space.shape[-1]\n",
    "\n",
    "print('###### Size of observation space ####### \\n', state_size)\n",
    "\n",
    "act_space = env.action_space\n",
    "\n",
    "print('###### Action space ####### \\n', act_space)\n",
    "\n",
    "act_size = env.action_space.shape[-1]\n",
    "\n",
    "print('###### Number of actions ####### \\n', act_size)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "print('##### State after reset ###### \\n', state)\n",
    "\n",
    "print('##### File where will be stored sigmas \\n', env.file_sigmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6da707",
   "metadata": {},
   "source": [
    "## Saving folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40786186",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir_ppo = 'models/PPO'\n",
    "models_dir_td3 = 'models/TD3'\n",
    "\n",
    "logdir = 'logs'\n",
    "\n",
    "if not os.path.exists(models_dir_ppo):\n",
    "    os.makedirs(models_dir_ppo)\n",
    "    \n",
    "if not os.path.exists(models_dir_td3):\n",
    "    os.makedirs(models_dir_td3)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927566a",
   "metadata": {},
   "source": [
    "## Twin Delayed DDPG (TD3) from `stable_baseline3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for DDPG\n",
    "action_noise = NormalActionNoise(mean=np.zeros(act_size), sigma=0.2 * np.ones(act_size))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, batch_size=64, gamma=1.0, verbose=1, seed=0\n",
    "            , tensorboard_log=logdir )\n",
    "\n",
    "# (policy, env, learning_rate=0.001, buffer_size=1000000,learning_starts=100, batch_size=100, \n",
    "# tau=0.005, gamma=0.99, train_freq=(1, 'episode'),  gradient_steps=- 1, action_noise=None, \n",
    "# replay_buffer_class=None, replay_buffer_kwargs=None,  optimize_memory_usage=False, \n",
    "# tensorboard_log=None, create_eval_env=False, policy_kwargs=None,  verbose=0, seed=None, \n",
    "# device='auto', _init_setup_model=True)\n",
    "\n",
    "model.learn(total_timesteps=300, log_interval = 5, n_eval_episodes = 1)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=4, eval_env=None, eval_freq=- 1,\n",
    "# n_eval_episodes=5, tb_log_name='DDPG', eval_log_path=None, reset_num_timesteps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4411e58",
   "metadata": {},
   "source": [
    "## PPO with GAE from `stable_baseline3` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "total_t_steps = 20\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, gamma = 1.0, tensorboard_log=logdir, batch_size=2, n_steps=2)\n",
    "\n",
    "# classstable_baselines3.ppo.PPO(policy, env, learning_rate=0.0003, n_steps=2048, \n",
    "#         batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2, \n",
    "#         clip_range_vf=None, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, \n",
    "#         use_sde=False, sde_sample_freq=- 1, target_kl=None, tensorboard_log=None, \n",
    "#         create_eval_env=False, policy_kwargs=None, verbose=0, seed=None, device='auto', \n",
    "#         _init_setup_model=True)\n",
    "\n",
    "for i in range(1,10):\n",
    "    model.learn(total_timesteps=total_t_steps, reset_num_timesteps=False)\n",
    "\n",
    "# learn(total_timesteps, callback=None, log_interval=1, eval_env=None, eval_freq=- 1, \n",
    "#       n_eval_episodes=5, tb_log_name='PPO', eval_log_path=None, reset_num_timesteps=True)\n",
    "\n",
    "    model.save(f\"{models_dir_ppo}/{total_t_steps*i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c58121",
   "metadata": {},
   "source": [
    "## From my `ddpg_agent.py` code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import DDPG_agent\n",
    "agent = DDPG_agent(state_size, act_size, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a586189",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all rewards, energies and princip dims in files during training\n",
    "def create_info_h5(agent, env):\n",
    "    # Check if file exist and creat it\n",
    "    i = 0\n",
    "    while os.path.exists(f'run_{i}.hdf5'):\n",
    "        i += 1\n",
    "    dataFile = h5py.File(f'run_{i}.hdf5', 'a')\n",
    "    \n",
    "    # Create dataset to store info in hdf5 file\n",
    "    info = {'alg':agent.name, 'env':env.unwrapped.spec.id}\n",
    "    st = h5py.string_dtype(encoding='utf-8')\n",
    "    dataFile.create_dataset('info', dtype=st)\n",
    "    for k in info.keys():\n",
    "        dataFile['info'].attrs[k] = info[k]\n",
    "\n",
    "    # Create dataset to store hyperparams of the model in hdf5 file\n",
    "    hyperparams = {'batch_size':agent.batch_size, 'bootstrap_size':agent.bootstrap_size \\\n",
    "                   , 'gamma':agent.gamma, 'tau':agent.tau,'lr_critic':agent.lr_critic \\\n",
    "                  , 'lr_actor':agent.lr_actor, 'update_every':agent.update_every \\\n",
    "                   , 'transfer_every':agent.transfer_every, 'num_update':agent.num_update \\\n",
    "                  , 'add_noise_every':agent.add_noise_every}\n",
    "    dataFile.create_dataset('hyperparams', dtype='f')\n",
    "    for k in hyperparams.keys():\n",
    "        dataFile['hyperparams'].attrs[k] = hyperparams[k]\n",
    "    \n",
    "    # Create group for rewards, energies, princip dims, actor and critic model\n",
    "    dataFile.create_group('sigmas')\n",
    "    dataFile.create_group('rewards')\n",
    "    dataFile.create_group('energies')\n",
    "    dataFile.create_group('princip_dims')\n",
    "    dataFile.create_group('actor_models')\n",
    "    dataFile.create_group('critic_models')\n",
    "    \n",
    "    return dataFile\n",
    "\n",
    "def save_all(dat_file, i_ep, sigmas_i_ep, rew_i_ep, en_i_ep, pri_dim_i_ep, act_model_i_ep, cr_model_i_ep):\n",
    "    # Create datasets for rewards, energies, pri dim and store data in it \n",
    "    dat_file['sigmas'].create_dataset(f'sigmas_ep_{i_ep}', dtype='f', data=sigmas_i_ep)\n",
    "    dat_file['rewards'].create_dataset(f'rew_ep_{i_ep}', dtype='f', data=rew_i_ep)\n",
    "    dat_file['energies'].create_dataset(f'en_ep_{i_ep}', dtype='f', data=en_i_ep)\n",
    "    dat_file['princip_dims'].create_dataset(f'pri_dim_ep_{i_ep}', dtype='i', data=pri_dim_i_ep)\n",
    "    \n",
    "    # Store in actor models group the network params at each ep\n",
    "    actor_model = torch.load(act_model_i_ep)\n",
    "    dat_file['actor_models'].create_dataset(f'act_mod_{i_ep}', dtype='f')\n",
    "    for k in actor_model.keys():\n",
    "        dat_file['actor_models'][f'act_mod_{i_ep}'].attrs.create(name=k,data=actor_model[k].cpu().data.numpy())\n",
    "    \n",
    "    # Store in actor models group the network params at each ep\n",
    "    critic_model = torch.load(cr_model_i_ep)\n",
    "    dat_file['critic_models'].create_dataset(f'cri_mod_{i_ep}', dtype='f')\n",
    "    for k in critic_model.keys():\n",
    "        dat_file['critic_models'][f'cri_mod_{i_ep}'].attrs.create(name=k,data=critic_model[k].cpu().data.numpy())\n",
    "\n",
    "def close_file(dat_file, actor_model_file, critic_model_file, file_sigmas):\n",
    "    dat_file.close()\n",
    "    os.remove(actor_model_file)\n",
    "    os.remove(critic_model_file)\n",
    "    os.remove(file_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc90e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run ddpg algs   \n",
    "def run_ddpg(max_t_step = 250, n_episodes=400):\n",
    "    \n",
    "    # Create h5 file and store info about alg and its hypereparams\n",
    "    dat_file = create_info_h5(agent, env)\n",
    "    \n",
    "    for i_ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        rew_i_ep = []\n",
    "        en_i_ep = []\n",
    "        pri_dim_i_ep = []\n",
    "\n",
    "        ## Training loop of each episode\n",
    "        for t_step in range(max_t_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Save rew, energies, princip dims, act and crit models\n",
    "            rew_i_ep.append(reward)\n",
    "            en_i_ep.append(state[0])\n",
    "            pri_dim_i_ep.append(env.princp_dim)\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        ## Save data during training (to not lose the work done)\n",
    "        save_all(dat_file=dat_file, i_ep=int(i_ep), sigmas_i_ep=env.actions_taken \\\n",
    "                 , rew_i_ep=rew_i_ep, en_i_ep=en_i_ep, pri_dim_i_ep=pri_dim_i_ep \\\n",
    "                 , act_model_i_ep='checkpoint_actor.pth', cr_model_i_ep='checkpoint_critic.pth')\n",
    "        \n",
    "        print('Episode {} ... Score: {:.3f}'.format(i_ep, np.sum(rew_i_ep)))\n",
    "\n",
    "    close_file(dat_file, 'checkpoint_actor.pth', 'checkpoint_critic.pth', env.file_sigmas)\n",
    "    return dat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cc27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [58.236084 36.820633 56.90082 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0257158\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2487993359676643\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [50.291565 40.510654 65.03306 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0252435\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2159558473083756\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [45.21618  34.25939  60.410885]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0250003\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.199043849760338\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [29.996693 31.964462 61.010174]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.00446907\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7713131284683357\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [56.987885 39.30943  81.42415 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.00405317\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7423916655562106\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [52.21992  40.476982 64.82669 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0039687\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.736517667065737\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [52.609764 66.62851  58.795258]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.000302538\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.43949806762869414\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [68.546326 68.14248  51.441162]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.000429229\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.4306880434454765\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [71.41677  80.855515 60.240818]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.00047566\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.42745925661096607\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [69.69955  78.301926 64.45448 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  -0.00140864\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.36258032785944927\n",
      "Episode 0 ... Score: -10.574\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [65.20325  62.540253 67.75856 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.022468\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.022949066080658\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [50.383205 83.104904 77.5709  ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0164898\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.6072282447743937\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [50.657684 90.580605 72.87634 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0164898\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.6072282447743937\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [55.661842 79.72524  65.26168 ]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0162718\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.5920686417091297\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [49.90474  55.823086 71.752594]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0122536\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.3126451690630407\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [70.03849  42.499542 86.62016 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0100341\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1583023295981096\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [90.57709 27.49602 74.65838]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00565611\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8538592533426108\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [85.829956 29.709099 68.89101 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00530333\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8293271213730495\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [71.548096 26.771824 55.350338]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.00045108\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.49190425910960656\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [91.97304  44.544327 44.121384]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  0.000426196\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.4901738390973289\n",
      "Episode 1 ... Score: -11.966\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [66.88027  54.917374 66.54063 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0233169\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.081981116732509\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [79.480316 49.201096 60.91075 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0232346\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.076258018878052\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [83.92812  35.691902 63.667175]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0146181\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.4770712307502816\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [90.45926  58.8267   58.480503]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0112487\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2427649620892591\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [72.87857  59.926247 70.55354 ]\n",
      "Basis size (it should be the same of full dim) =   5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  0.0112388\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2420765213995963\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [79.3582   45.096256 61.160923]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0104849\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1896507197899506\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [96.427505 38.841225 71.24548 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.0104566\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1876827529700105\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [76.66647 34.76061 65.12565]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00721334\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.9621481922477599\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [70.09727  33.811424 74.0064  ]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.00631522\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8996934091974129\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [59.14389  54.178413 66.76767 ]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  0.00595105\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.8743692227374247\n",
      "Episode 2 ... Score: -13.234\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [66.54201  51.53026  45.022285]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0261982\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2823451732093307\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [57.40828  56.04046  56.868057]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0256105\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.2414768304503596\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [54.163784 35.657013 65.11802 ]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.010289\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1760279388702912\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [75.83159 43.55326 74.22288]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0102635\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1742546825484386\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [66.7879   35.89921  53.130432]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.010107\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1633717564947048\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [66.426254 27.333567 55.080063]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00458014\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7790368766906255\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [83.22603    6.5205307 67.84433  ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  -0.0154982\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.6172001169306061\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [78.90448   3.56736  63.878735]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  -0.0165501\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  0.6903486786936899\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [75.45174   0.       79.621155]\n",
      "**** ILLEGAL ACTION **** --> Set reward: -10.0\n",
      "This action IS REMOVED from actions taken and sigmas, the energy is NOT STORED!\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [61.204163 11.167725 78.160934]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  -0.0246898\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  1.2563790505759638\n",
      "Episode 3 ... Score: -16.253\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [61.784767 45.459106 80.065186]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.023257\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.0778157028627398\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [61.585163 68.99784  74.271065]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0213642\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.9461914061566254\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [72.76434  73.828636 55.824623]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0182412\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.7290196613271664\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [73.18293 78.32175 70.26387]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0118121\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2819434958826985\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [72.872505 76.07813  70.11363 ]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.0116036\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2674445177216551\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [83.32714  55.447807 65.26494 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.0111137\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.2333771345029714\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [68.76483  51.624626 53.16461 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00978988\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.1413194017971975\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [79.55551  53.649284 57.803112]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00840313\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0448855506469403\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [81.083755 56.966545 27.719273]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.0037471\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.7211077219315047\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [62.723118 40.026993 34.23876 ]\n",
      "Basis size (it should be the same of full dim) =   10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this action the energy is:  0.0033804\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.6956076006286036\n",
      "Episode 4 ... Score: -13.139\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [64.1879   55.270073 61.90925 ]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0242055\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.143773884089363\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [47.73578  54.236473 58.88013 ]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0240952\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.136103681254049\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [36.66607  41.726643 57.474506]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0125205\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.3312052518984512\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [41.952194 41.304867 57.638733]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.00929772\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.107094859390772\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [35.5551   51.25997  61.831844]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.00897332\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0845362573982413\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [45.510914 45.497993 58.57912 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00284322\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.6582523915708105\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [60.34382  42.482044 46.43509 ]\n",
      "Basis size (it should be the same of full dim) =   7\n",
      "With this action the energy is:  0.00213007\n",
      "With this action the full dim is:  7  and princip dim is:  7\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.6086603231029493\n",
      "#### CALL STEP #### 8\n",
      "Action chosen at step:  [53.628433 56.636047 40.23886 ]\n",
      "Basis size (it should be the same of full dim) =   8\n",
      "With this action the energy is:  0.00131706\n",
      "With this action the full dim is:  8  and princip dim is:  8\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.5521240437997808\n",
      "#### CALL STEP #### 9\n",
      "Action chosen at step:  [39.830666 49.73127  43.019253]\n",
      "Basis size (it should be the same of full dim) =   9\n",
      "With this action the energy is:  0.000834039\n",
      "With this action the full dim is:  9  and princip dim is:  9\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.518535022551216\n",
      "#### CALL STEP #### 10\n",
      "Action chosen at step:  [45.38184  52.69643  52.434975]\n",
      "Basis size (it should be the same of full dim) =   10\n",
      "With this action the energy is:  0.000536569\n",
      "With this action the full dim is:  10  and princip dim is:  10\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -0.49784911831348744\n",
      "Episode 5 ... Score: -10.638\n",
      "#### CALL RESET ####\n",
      "Action chosen at reset:  [0.]\n",
      "Actions taken at reset:  []\n",
      "Energies got at reset:  [0.0]\n",
      "#### CALL STEP #### 1\n",
      "Action chosen at step:  [66.11961  58.710762 49.244686]\n",
      "Basis size (it should be the same of full dim) =   1\n",
      "With this action the energy is:  0.0250027\n",
      "With this action the full dim is:  1  and princip dim is:  1\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -2.1992107444729836\n",
      "#### CALL STEP #### 2\n",
      "Action chosen at step:  [64.30253  53.494545 54.433056]\n",
      "Basis size (it should be the same of full dim) =   2\n",
      "With this action the energy is:  0.0190778\n",
      "With this action the full dim is:  2  and princip dim is:  2\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.787196376576711\n",
      "#### CALL STEP #### 3\n",
      "Action chosen at step:  [45.091473 61.052574 51.743904]\n",
      "Basis size (it should be the same of full dim) =   3\n",
      "With this action the energy is:  0.0190103\n",
      "With this action the full dim is:  3  and princip dim is:  3\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.782502462783567\n",
      "#### CALL STEP #### 4\n",
      "Action chosen at step:  [82.820435 40.359016 59.363003]\n",
      "Basis size (it should be the same of full dim) =   4\n",
      "With this action the energy is:  0.0182697\n",
      "With this action the full dim is:  4  and princip dim is:  4\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.7310015360398268\n",
      "#### CALL STEP #### 5\n",
      "Action chosen at step:  [84.23723  39.154587 68.352776]\n",
      "Basis size (it should be the same of full dim) =   5\n",
      "With this action the energy is:  0.016226\n",
      "With this action the full dim is:  5  and princip dim is:  5\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.5888837342761502\n",
      "#### CALL STEP #### 6\n",
      "Action chosen at step:  [71.71082  31.585955 68.06983 ]\n",
      "Basis size (it should be the same of full dim) =   6\n",
      "With this action the energy is:  0.00896937\n",
      "With this action the full dim is:  6  and princip dim is:  6\n",
      "#### THE ACTION IS A GOOD ONE #### --> Store the energy got!\n",
      "Reward is  -1.0842615765170116\n",
      "#### CALL STEP #### 7\n",
      "Action chosen at step:  [61.52725  38.777115 61.072968]\n",
      "Basis size (it should be the same of full dim) =   7\n"
     ]
    }
   ],
   "source": [
    "all_data = run_ddpg(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9d72c",
   "metadata": {},
   "source": [
    "## Random search as in original SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "scores = []\n",
    "step = 0\n",
    "score = 0.0\n",
    "\n",
    "while True:\n",
    "    print(\".....STEP.....\", step)\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    step = step + 1\n",
    "    score += reward\n",
    "    scores.append(score)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
